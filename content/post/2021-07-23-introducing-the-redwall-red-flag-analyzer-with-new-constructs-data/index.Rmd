---
title: "Introducing the Redwall Red Flag Analyzer with New Constructs Data"
author: "David Lucey"
date: '2021-07-23'
slug: introducing-the-redwall-red-flag-analyzer-with-new-constructs-data
categories: ["R"]
tags: ["XBRL", "quantmod", "dataviz", "shiny"]
draft: true
---

![Red Flag Explorer](images/red_flag_app.png)


# Introduction

A few months ago in [A Blueprint of "Red Flag" alerts Using Adjusted Earnings Data](https://redwallanalytics.com/2021/04/21/a-blueprint-of-red-flag-alerts-using-adjusted-earnings-data/), we mused about using fantastic New Constructs historical data to back-test the 2003 CFA Conference Proceedings [Revelations in Financial Reporting](https://www.tandfonline.com/toc/ufaj20/current) by Bruce Gulliver. This article gave the hypothesis that impending difficulties could be forecast with a simple set of financial statement metrics, or "red flags". That article always stuck with us, and especially now that R has become a daily tool, this opportunity was too big to resist.

After our blog post, New Constructs kindly offered to let us use their data to test out the theory on thousands of companies over 20 years. In this post, we will describe our analysis and describe the interactive [Red Flag Explorer](https://luceyda.shinyapps.io/redflagapp/) Shiny app we have built for anyone who would like to interact with the data.

# Methodology

The methodology for generating "red flags" was close to what we laid out in [A Blueprint of "Red Flag" alerts Using Adjusted Earnings Data](https://redwallanalytics.com/2021/04/21/a-blueprint-of-red-flag-alerts-using-adjusted-earnings-data/). One difference was with Turnover where we factored in sector when determining companies with unfavorable metrics. The other major difference was in the cash flow metric, where we never quite understood the "Revelations" adjustments to operating cash flow. The major adjustment from the article had to do with cash flow differences related to the expensing of employee share options. We are not sure this if this is still significant problem, and didn't have data from New Constructs to calculate it anyhow. If we were to do it again though, we would have requested the data to calculate the difference between New Constructs "True Free Cash Flow" and "Traditional Free Cash Flow" as laid out in [The Most Overstated And Understated FCF In The S&P 500 Post 1Q21 Earnings](https://www.forbes.com/sites/greatspeculations/2021/06/16/the-most-overstated-and-understated-fcf-in-the-sp-500-post-1q21-earnings/?sh=536bff042be2) by David Trainer in Forbes in June (after we had already requested the data). We also calculated the trend-related variables (ie: increasing days of inventory/receivables, and declining margins and returns) taking more than one year of change into account to give an added penalty to and extended negative trend. We also added "red flags" for companies with amended filings and companies with more than 2 flags showing filing-on-filing increases in total flags. With the exception of amended filings, most of the "red flags" were calibrated to occur in about 20% of the cases. There was no special knowledge consideration given to an "informed" level where past evidence supported likely problems, just that it was deviating negatively relative to the large majority of others.

In order to calculate ex post quarterly returns, we collected weekly prices for every company using the R `{BatchGetSymbols}` package (which uses Yahoo as its source). About 1,600 companies were not available by this method, so we switched to `{quantmod}` using Alpha Vantage, where we were able to collect the closing price histories of an additional \~800 companies. While Yahoo maintains, and we used the "adjusted prices" (ie: for splits, dividends and corporate actions) for our analysis, Alpha Vantage only had the closing price. Although we assume that many of the residual companies collected from Alpha Vantage would have gotten into difficulty and been de-listed, while others would have been subsumed into other companies. If the stock price went to zero or was otherwise de-listed from trading, we think the fact that it was not adjusted is likely to be less relevant. All in all, we had almost 5 million prices on 5,000 companies, and we calculated the log returns over 1 through 13 quarters relative to the Vanguard Total Market Index fund ("TMI").

In most single years, this amounts to several thousand stocks so on average, across all the periods and companies, we expect the aggregated relative returns we are showing to be good, but not perfect, representations of the true returns of those groups. If a user of the app filters down into a smaller group of measurements, this will become less true. In addition, we expect the 800 companies we were unable to collect returns were likely perform less well than the rest of the companies. In addition, we calculated that the companies we were unable to match to returns had a higher average number of "red flags" on these stocks. This means that we our returns may be biased on the high side because of "survivor bias", and we guess if anything, we are likely to be underestimating the negative effects of high "red flags" groups.

# Using the Shiny App

The triple horizontal line icon at the top left allows the user to change any of the inputs to the app. The "View" switch allows to toggle between 67,000 annual and 78,500 quarterly filings. It should be noted that a flag on the same filing, for the same company and period, might differ on the annual and quarterly basis, because they are calculated with reference to the other filings in that group. It is possible to filter by sector or by selecting a customized group of companies. This enables the user to see the aggregated return metrics for the selected items in the "Summary" table on the left. The chart on the right shows the evolution of the metrics over time. The "Summary Plot" loses its meaning if there are only a few items, because the same company will most likely fall into multiple "red flag" groups over time. If the selection is small enough, there might not even be a data point for a higher number of flags in a given year, and as a result, there may be an extended time difference between two points when there were no members of that group between those periods (usually 5+ flags in a period).

When the app loads, it includes the aggregate for all 13 quarterly return periods, but we would not advise to look at it in this way, because these periods overlap each other (ie: the one quarter return is part of the two quarter return). The quarterly data set does not show beyond eight quarters, and though the annual data set shows up to 13 quarters, using this many will result in the loss of data in 2020 and 2021, because we have to leave that much lead time. The "Flags" tab shows the descriptions and counts of all "red flags" in the filtered selection. As mentioned earlier, most flags occur in approximately 15-20% of the cases, so seeing more than a few tends to be the exception. The number of unique companies included in the "red flag" group is a more important indicator, and when looking at just one return period, this number should be pretty close to the number of reports unless an unusually long quarterly return period is selected.

# Cursory Analysis

The beauty of our Shiny app is that we are able leave it for the user to judge for themselves. In aggregate considering all periods, we observe that returns tend to be stable between 0 and 2-3 flags, but fall off and become more volatile with a higher number of flags. This trend tends to be visible and persistent for most sectors and periods, although there are exceptions. The pattern seems a little more visible in the cyclical sectors. The pattern seems less pronounced in the quarterly data, but this starts 13 years later. If later years are selected with the filters, the difference in median and mean returns with higher and lower flags seems to be smaller and both are more negative relative to the index.

Leaving the "red flags" aside, we were surprised to see that at the beginning of the period, the average stock in most "red flag" groups outperformed, but over time most trailed the TMI. In the year 2000 (the very beginning of our data), there was a more significant number of companies with 5-7 red flags, and they really under-performed the lower red flag groups by a lot. We recall the post technology bubble period as being a golden age of stock picking. After 2004 or 2005, almost all red flag groups under-performed the TMI. Certainly, when Mr. Gulliver wrote his piece when the difference was more striking, so maybe more market observers took note than we thought. Interestingly, 2020 seems to be the only year when all "red flag" groups (from 0-7) actually outperformed the TMI, and the higher total flag groups outperformed lower groups, so this year in particular looks exceptional.

# Fun Themes to Filter

As mentioned earlier, the app can be sorted by sector and year, but we also suggest trying some themes by specifying individual tickers. As we mentioned earlier, it is important to give a significant number of tickers, and even these recommendations may be too few. One group might be high internally generated intangible companies which have defied valuation-based predictions in the recent periods. In this group, we might include FB, GOOGL, NFLX, AMZN, SHOP, MSFT, APPL, BABA, UBER, LYFT, TWTR, SNAP, TCEHY, TSLA, ZM, CRWD, PINS, PTON, PDD, MRNA, GRUB, TDOC, DOCU, SFIX, SPOT, INTU & CRM (and hopefully RBLX and SNOW in the future if covered in the future). Another interesting list of covered names would be a selection of "Meme" and penny stocks found on [WSB's Hottest Meme Stocks](https://memestocks.org): BBBY, BB, GME, AMC, CLNE, CLF, A, X, NIO, EDU, TAL, DTE, ACOR, CCJ, BIO, SQ, CC, SPRT and FLT.

# Next Steps

There are so many possible next steps it is difficult to know where to start. The model proposed by the CFA Conference Proceedings was rules-based, aggregating a large number of weak boolean signals. It's power comes from systematically taking more elements over a larger number of companies than any single analyst could ever consider at the same time. It isn't clear if the relationship between the number of flags and returns should be linear, and the few very high number of flags showed a bigger than linear impact on returns. The statistical modeling tools we have learned so far rely on finding a few highly significant numeric variables, so we are not quite sure how to build a model primarily from weak variables where the signal comes from aggregation.

We have included mainly accounting ratios in our analysis so far, but we could add variables from other data sources, such as NLP signals such as sentiment scores from the MD&A, management incentive alignment measures, short interest or insider selling. At the moment out of almost 145,000 filings, we found only about 400 with six or more "red flags", so it might be beneficial to take into consideration more risk factors.

The nature of the data is also a consideration. We would like to measure our confidence that a certain number of "red flags" has been better or worse than another level. The challenge is that we have the longitudinal aspects of the data, and also that there are a lot fewer cases of companies with a high number of "red flags", and the returns become much more variable in those cases. This is a situation best addressed with a Bayesian mixture model, which would allow us to get a posterior for each "red flag" taking into account measurements during other periods and gauge our confidence in the differences of the means of the "red flag" groups. This is likely something we will try in the future, but it is still beyond our capabilities.

Another aspect of the analysis is that by collapsing the ratios down into boolean "red flags", we surrendered a lot of data and are not able to allow the component variables to interact. We are even treating the aggregated variables equally when some may be more important than others, or may have more importance when interacted with others. New Constructs was kind enough to share their data, but that was on a temporary basis, so this is likely beyond what we can achieve, but we may be able to look more closely at the interactions among the variables we have already calculated.

# Conclusion

When we wrote [A Blueprint of "Red Flag" alerts Using Adjusted Earnings Data](https://redwallanalytics.com/2021/04/21/a-blueprint-of-red-flag-alerts-using-adjusted-earnings-data/), it felt like our proposal was out on a limb compared to what might be achievable. Thanks to the generosity of New Constructs and the powers of R, we have taken a long-held personal research question, and built a working prototype which we can share with others and possibly add onto over time. It is possible that the disappearance of the "red flag" return spread may be an indication that others have already done this, but if it exists, we haven't seen that work in public. We will include the code for our calculations and app on Github in the future. Our work is only a back-test, so we encourage others to subscribe to New Constructs API and
