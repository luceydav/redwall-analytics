---
title: Loading a large, messy csv using data.table fread with cli tools
author: R package build
date: '2022-02-06'
slug: loading-a-large-messy-csv-using-data-table-fread-with-cli-tools
categories:
  - R
tags:
  - data.table
  - csvkit
  - xsv
  - scrubcsv
---


<details>

<summary> Setup </summary>
```{r 'setup'}
library(data.table)
library(here)
library(glue)
library(tictoc)
setDTthreads(percent = 90)
```
</details>

```{r 'paths', message=FALSE, warning=FALSE, include=FALSE}
path_to_data <- "~/Desktop/David/Projects/uscompanies/data"
path_to_original <- here::here(path_to_data, "uscompanieslist.csv")
```

# Introduction

On a recent side project, we encountered a large (7GB) csv of 30+ million US business names and addresses, which couldn't be loaded into R, because of corrupted records. While not widely discussed, we have known for some time that it was possible to pipe command line instructions into `{data.table}`'s `fread()` by using its "cmd" parameter. However, there were only a few snippets available about how to do this, and most of these were constrained to limited strategies using `awk`. There were a few times in the past that we used `awk`, and we sometimes even got it to work, though we often didn't understand why. `awk` seems like a great tool, but is like learning an entirely new language.

When we discovered Jeroen Janssens' [Data Science at the Command Line](https://datascienceatthecommandline.com) a few months ago, we realized there were a lot more possibilities for solving problems like this one. This book helped us to understand that the strategy of using `fread()`'s cmd capability might be expanded beyond `awk`. Unfortunately, the data set does not belong to us, so we cannot share it, but we will demonstrate the methods in case helpful for others.


# Tools and Setup

The specific tools we been learning are {`xsv}`, `{rg}` (ripgrep), `csvkit` and `scrubcsv`. The first two were developed by [BurntSushi](https://github.com/BurntSushi) using Rust, `{csvkit}` is a Python package, and `{scrubcsv}` is another Rust package inspired by the first two. We quickly learned that this tool set is a lot easier to install on Mac than Windows (using WSL), because most can be installed with Homebrew, the Mac package manager. We were not able to figure out how to install `{xsv}` and `ripgrep` on WSL, but "brew install xsv" and "brew install ripgrep" easily installed the libraries on our Mac.

Since we started our data journey about 5 years ago, managing Python installations has always been a challenge, and we will not discuss this here. Once Python is set up, the third is easy with "pip install csvkit". Lastly, `{scrubcsv}` requires one step further, because there is no Homebrew formula, so first Rust and its package manager cargo had to be installed, which again can be accomplished with Homebrew following these [instructions](https://www.chrisjmendez.com/2022/02/22/installing-multiple-versions-of-rust-on-your-mac-using-homebrew/). Once installed, `{scrubcsv}` only requires "cargo install scrubcsv".

Of the tools, `{rg}` is grep on steroids, while `{xsv}` and `{csvkit}` have many similar capabilities to slice and dice a csv. Though `{xsv}` is a significantly faster, `{csvkit}` has a built in `cleancsv` capability which can be used to solve our problem. `{scrubcsv}` does only one thing, it drops rows with the wrong number of columns, and it does this very fast. This seems like a more limited solution, but in our case it turns out to be just the ticket.

# The Problem

As shown below, when we try to load the data set, we get "Error in fread("/Users/davidlucey/Desktop/David/Projects/uscompanies/data/uscompanieslist.csv", : R character strings are limited to 2\^31-1 bytes". We were not the only ones who have encountered this cryptic error, but it seemed the main way to solve it as outlined in this SO post <https://stackoverflow.com/questions/68075990/loading-csv-with-fread-stops-because-of-to-large-string>, is to ask the owner to reformat it, which wasn't an option.

```{r 'fread-error-example-1', echo=TRUE, message=FALSE, warning=FALSE}
# Unsuccessful code
try(fread(path_to_original))
```

As the SO poster was asking, it would be nice to be able to instruct `fread()` to try each and skip the bad rows, but this is not possible (at least from what we have figured out so far). We didn't know which or how many rows specifically were causing the problem. Since the data set was so large, finding the problem, rows felt like a needle in a haystack, and the usual solution of loading it all into memory and looking around wasn't possible.

# Using `csvclean`

Like many who were previously scared by the CLI, the first step was to get over the fear of the help manual, `cleancsv` shown below.

<details>

<summary> csvclean manual </summary>
```{bash 'csvclean-manual', echo=TRUE}
csvclean -h
```
</details>

As we discovered is often the case with UNIX tools, there were not as many walk-through detailed examples of `{csvkit}` as with many R packages. We found this one particularly cryptic as it seemed unclear about its output, but in hindsight, the -n command mentions "output files" which are created. We were concerned that it might alter our data, so created a backup and ran against that.

```{r 'csvclean', eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Run previously to verify working, output on disc
system(command = glue::glue("csvclean {path_to_original}"))
```

After about an hour, the final output gives two new csv's ("uscompanieslist_err.csv" and "uscompanieslist_out.csv") by default, and leaves the original intact (uscompanieslist.csv). This is good, but means there is a need for a lot of disc space.

```{r 'csvclean-output-files', echo=FALSE, message=FALSE, warning=FALSE}
list.files(path_to_data, pattern = "uscompanieslist")
```

# Bad Rows

In "uscompanieslist_err.csv", `csvclean` adds two columns, one of which specifies the actual number of rows versus the number expected in each row. It also contains the line number of the original file where the problem was happening, which would have been nice to have earlier while we were hunting for bad rows. The cadence of our bad rows, which is every few thousand, can be seen and why our efforts at trying to load in chunks was problematic (chunks of a few thousand rows in 30 million).

<details>

<summary> `{csvclean}` Bad Rows </summary>
```{r 'csvclean-err', echo=TRUE, message=FALSE, warning=FALSE}
data <- 
  fread(here::here(path_to_data, "uscompanieslist_err.csv"), select = 1:2, nrows = 10)
data
```
</details>


This file still contains rows with a differing number of columns, so still cannot be read by `fread()`. Here we use `{rg}` to filter out the remaining bad rows and `{xsv}` to drop the `csvclean`'s metadata columns, piped into `fread()`. In our case, most of the intact rows have 22 columns, instead of the expected 28, so we are guessing this data was somehow tacked on from another source. Although we use `{rg}` again here, we could have used `grep` and it probably wouldn't have been much difference for 1 million rows, but it could also be done with any of the other tools or even with a traditional `grep`, also in about 10 seconds.


```{r 'load-bad-csvkit', echo=TRUE, message=FALSE, warning=FALSE}
tic()
# Load bad_csvkit_data
bad_csvkit_data <- 
  fread(cmd = glue::glue(
    "rg '22 columns' { here::here(path_to_data, 'uscompanieslist_err.csv') } | xsv select 3-13,17,19,20-21,24"))
toc()
```

As shown above there are `r nrow(bad_csvkit_data)` in the data set, and column names are lost and have to be manually re-inserted. At first, we were worries that the columns would be badly formatted, mistakenly merging columns, but looking at random samples of rows, this was not the case. A faster alternative with `{scrubcsv}`. There are also several columns which are missing all data or almost all blank cells. We can also add NULL columns for the ones which are missing.

<summary> Add table names </summary>
```{r 'table-names', echo=TRUE, message=FALSE, warning=FALSE}
# Column names
data_names <- c(
    "COMPANY_NAME",
    "SIC_CODE",
    "SIC_DESCRIPTION",
    "ADDRESS",
    "CITY",
    "STATE",
    "ZIP",
    "COUNTY",
    "PHONE",
    "FAX_NUMBER",
    "WEBSITE",
    "EMPLOYEE_RANGE",
    "SALES_VOLUME_RANGE",
    "CONTACT_FIRSTNAME",
    "CONTACT_LASTNAME",
    "CONTACT_TITLE"
  )
names(bad_csvkit_data) <- data_names
sample <- bad_csvkit_data[sample(5)]
```
</details>

```{r 'sample-bad-csvkit', echo=TRUE}
# Quick view of final data
sample
```


As we mentioned above, `csvclean` took about an hour to run, but there are probably many other ways to accomplish our goal. Although we didn't know the exact problem when we first tried `csvclean`, with hindsight, a better solution would have been `{scrubcsv}`, because it drops the rows with a differing number of columns, and it does so very quickly. One missing feature of `{scrubcsv}` is the lack of an output for the bad rows, so we had to capture these in the second line using the CLI `comm` command. In order not to fill up my disc further, these are not run here, but the total time to run both processes was just 5 minutes, and with a little cleaning, yields the same csv's as `{csvkit}`, which took an hour.

![CLI to Replicate `{csvclean}` with `{scrubcsv}`](images/Screen%20Shot%202022-04-21%20at%2012.47.52%20PM-01.png)

Like the bad_csvkit_data, the output of bad_scrub_data still has a few rows with the wrong number of columns, but those are easily dropped with another run of `csvscrub` to remove all of the rows which do not have the predominant 22 columns, and using `{xsv}`, we also drop empty columns with `{xsv}` select.


```{r 'clean-scrub-data', echo=TRUE, message=FALSE, warning=FALSE}
# Filter, select and load with fread
bad_scrub_data <- 
  fread(cmd = glue::glue("scrubcsv {path_to_data}/bad_scrub_data.csv | xsv select 1-11,15,17-19,22"))

# Use same names
names(bad_scrub_data) <- data_names
  
# Check if identical
identical(bad_csvkit_data, bad_scrub_data)
```

# Further Explorations

Here we show off a few tricks, with this trick scanning to locate Connecticut businesses from the 30 million rows in less than a minute. For example, we are able to stack the two data sets, filter the State of Connecticut and calculate the number of businesses by city. We would have liked to call the output from \`fread()\`, but in this case, the sub-processes from stacking the two tables seem to not be able to find the file paths from within R, so that is the first example of something which doesn't work.

```{bash}
time xsv cat rows <(xsv select 1,5,6 ~/Desktop/David/Projects/uscompanies/data/scubbed_data.csv) <(xsv select 1,5,6 ~/Desktop/David/Projects/uscompanies/data/bad_scrub_data.csv) | xsv search -s STATE 'CT' | xsv frequency -s CITY
```

We can count the top 10 most states occurring in the data using `xsv frequency` and choosing the STATE column, which takes about a minute. The count seem roughly as expected, but a business in this data set can range from a sole proprietor to a multi-national. What we are really seeing is the number of locations which are a business.

```{r 'states', echo=TRUE, message=FALSE, warning=FALSE}
tic()
data <- 
  fread(cmd = glue::glue('xsv select STATE {path_to_data}/scubbed_data.csv | xsv frequency'))
toc()
data
```

For a grand finale, we thought it might be nice to find unique rows, but interestingly, we couldn't find this as a built in capability in either `{xsv}` or `{csvkit}`, though both have requests to add it. The traditional sort \| uniq would be pretty slow for such a large data set on our small computer, so we found another Rust library `{huniq}`. Now in the hang of it, there are so many resources available. It looks like if looked at by zip, it took about a minute to find out that there are 26 million unique businesses in the stacked data set, less than the full listed 31 million.

```{bash}
time xsv cat rows <(xsv select 1,7 ~/Desktop/David/Projects/uscompanies/data/scubbed_data.csv) <(xsv select 1,7 ~/Desktop/David/Projects/uscompanies/data/bad_scrub_data.csv) | huniq | xsv count
```

# Conclusion

R is so often knocked for being slow, but views as wrapper of other tools like the Rust libraries, it might not be so true. `{xsv}`, `{rg}` and `{huniq}` were not as hard for us to understand as `awk` and surely perform a lot better. This exercise improved our confidence with the command line, and the tricks from Data Science at the Command Line. After a while referring to the man(ual) or help pages made, along with the usual Google search and Stack Overflow, we were able to figure out most challenges. Combined with `fread()`, it really starts to seem like a superpower at least with large, messy data sets. We are hoping that connecting the dots here will help others to solve similar problems.
