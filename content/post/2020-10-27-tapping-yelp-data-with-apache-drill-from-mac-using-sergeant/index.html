---
title: Tapping Yelp data with Apache Drill from Mac using {sergeant}
author: David Lucey
date: '2020-10-27'
slug: tapping-yelp-data-with-apache-drill-from-mac-using-sergeant
categories: ["R", "Code-Oriented", "SQL"]
tags: ["Apache Drill", "Java"]
---

<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<iframe src="https://www.yelp.com/dataset" width="672" height="400px">
</iframe>
<details>
<p><summary>Click to see package details</summary></p>
<pre class="r"><code># Libraries
packages &lt;- 
  c(&quot;tidyverse&quot;,
    &quot;sergeant&quot;,
    &quot;tictoc&quot;
    )

if (length(setdiff(packages,rownames(installed.packages()))) &gt; 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

invisible(lapply(packages, library, character.only = TRUE))

knitr::opts_chunk$set(
  comment = NA,
  fig.width = 12,
  fig.height = 8,
  out.width = &#39;100%&#39;
)</code></pre>
</details>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>At Redwall, we have been in nonstop exploration of new data sets over the last couple of years. As our data grows and the targets of interest get bigger, we have been finding the old method of loading csv’s from disc, and working on the full set in memory is becoming less optimal. We thought we would try Apache Drill via the <code>{sergeant}</code> package created by Bob Rudis, a prolific R developer. Apache Drill seems amazing because it would allow us to be agnostic as to data source and type. Usually, we write blog posts to show off things we have learned which are actually working. The punchline in this case though, is that we were not able to get where we hoped so far with Drill. We will chronicle what we have done so far, and where we are still falling short.</p>
<p>Recall in <a href="https://redwallanalytics.com/2020/10/12/finding-the-dimensions-of-secdatabase-com-from-2010-2020-part-2/">Finding the Dimensions of <code>secdatabase.com</code> from 2010-2020 - Part 2</a>, we were able to query a data set which was over 20GB with the AWS Athena query service with pretty much instant response. With Apache Drill on one node on our 2005 Apple iMac with 8GB of RAM, queries with a couple of joins and some aggregation were taking at least 30 minutes, but usually much longer on a much smaller data set (if they didn’t crash our computer altogether). This could well be our machine, something we did wrong in configuring, poor query management or all of the above. We are writing this post in hopes of a response from experts, as well as to help others who might be trying to understand how to use Java, Drill or even the command line from RStudio. We promise to update the post with any feedback, so that it provides a pathway to others seeking to do the same.</p>
</div>
<div id="yelp-academic-data-set" class="section level1">
<h1>Yelp Academic Data Set</h1>
<p>We were hoping to avoid downloading it, but then we found Bob Rudis’ excellent <a href="https://rud.is/rpubs/yelp.html">Analyzing the Yelp Academic Dataset w/Drill &amp; sergeant</a> blog post and became intrigued by the possibility of having the flexible connection it offered, agnostic about storage and data formats. The Yelp Academic data set is about 10GB in size and took us over an hour to download, and are summarized in the image from the web page above. We hoped that we might be able to use it to explore the death rate of businesses in areas with differing COVID-19 mask and other non pharmaceutical interventions. Unfortunately, this is not possible at the moment, because it only runs through the end of 2019. The files are all in JSON format, and were one of the original examples given on the Apache Drill website and with the {sergeant} package. Shown below, the “business” file is the smallest, and “reviews” are by far the largest. Users visit businesses and give reviews, check-ins or tips, so the two main identifiers which tie the tables together are business_id and the user_id. There is a lot of opportunity to practice joins and aggregations if you can get it to work.</p>
<details>
<p><summary>Click to see code generating output</summary></p>
<pre class="r"><code>d &lt;-
  file.info(
    list.files(
      path = &quot;/Volumes/davidlucey/aDL/data/yelp_dataset/&quot;,
      pattern = &quot;.json&quot;,
      full.names = TRUE
    )
  )[c(1, 2)]
data.frame(file = stringr::str_extract(rownames(d), &quot;yelp.*&quot;), size = d$size)</code></pre>
<pre><code>                                               file       size
1 yelp_dataset//yelp_academic_dataset_business.json  152898689
2  yelp_dataset//yelp_academic_dataset_checkin.json  449663480
3   yelp_dataset//yelp_academic_dataset_review.json 6325565224
4      yelp_dataset//yelp_academic_dataset_tip.json  263489322
5     yelp_dataset//yelp_academic_dataset_user.json 3268069927</code></pre>
</details>
<pre><code>                                               file       size
1 yelp_dataset//yelp_academic_dataset_business.json  152898689
2  yelp_dataset//yelp_academic_dataset_checkin.json  449663480
3   yelp_dataset//yelp_academic_dataset_review.json 6325565224
4      yelp_dataset//yelp_academic_dataset_tip.json  263489322
5     yelp_dataset//yelp_academic_dataset_user.json 3268069927</code></pre>
</div>
<div id="background-on-drill" class="section level1">
<h1>Background on Drill</h1>
<p>To quote from this guide: <a href="https://www.tutorialspoint.com/apache_drill/apache_drill_quick_guide.htm">Apache Drill - Quick Guide</a>.</p>
<p><em>Apache Drill is a low latency schema-free query engine for big data. Drill uses a JSON document model internally which allows it to query data of any structure. Drill works with a variety of non-relational data stores, including Hadoop, NoSQL databases (MongoDB, HBase) and cloud storage like Amazon S3, Azure Blob Storage, etc. Users can query the data using a standard SQL and BI Tools, which doesn’t require to create and manage schemas.</em></p>
<p>We also found the excellent chart shown in <em>SQL on Everything with Apache Drill</em> below on <a href="https://technology.amis.nl/2019/03/11/what-is-apache-drill-and-how-to-setup-your-proof-of-concept/">What is Apache Drill and how to setup your Proof-of-Concept</a></p>
<div class="figure">
<img src="/img/drill/sql-on-everything.png" alt="" />
<p class="caption">SQL on Everything with Apache Drill</p>
</div>
<p>If this could work, it feels like we could fire it up and use it in just about any of our data sources or types. In this post, we are just going to use with a single node as we are only working with one small computer, but it looks like it should be easy to add additional nodes to speed things up.</p>
</div>
<div id="sergeant" class="section level1">
<h1>Sergeant</h1>
<p>As usual, none of this would have been possible without an amazing open source package created and shared by a real developer, often in their free time. In this case, we relied on Bob Rudis’ (Drill) <code>{sergeant}</code> package, blog posts and bookdown manual <a href="https://rud.is/books/drill-sergeant-rstats/drill-in-more-than-10-minutes.html">Drill in More than 10 Minutes</a>. He explains that he set up the interface because he saw Drill as a streamlined alternative to SPARK for those not needing the ML components (ie: just needing to query large data sources of disparate types like json, csv, parquet and rdbms). The package allows to connect to Drill via <code>dplyr</code> interface with the <code>src_drill()</code> function, and also the REST API with <code>drill_connection()</code>. Before using <code>{sergeant}</code> though, Java, Drill and Zookeeper must be installed.</p>
</div>
<div id="java" class="section level1">
<h1>Java</h1>
<p>Drill requires Oracle JDK 1.8, which is several generations earlier than the version we currently have installed on our Mac. In our first year or two, we tangled with Java because we really wanted to use <code>{tabulizer}</code> to extract tables from pdfs. We burned a lot of time trying to understand the versions and how to install and point to them on Stack Overflow. Just last week, we saw a post looking for advice on loading the <code>{xlsx}</code> package, which depends on Java, as well. One of the magical discoveries we made was <a href="https://www.jenv.be">Java Environment</a>. Go to <a href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html">Java SE Development Kit 8 Downloads</a>, choose the latest Mac Version of 1.8, and install the .dmg. Then on a Mac, <code>brew install jenv</code>, and it is off to the races. Here we show the Java versions on our machine from the Terminal.</p>
<pre class="bash"><code>jenv versions</code></pre>
<pre><code>  system
  1.6
  1.6.0.65
  1.8
  1.8.0.261
  14
  14.0
* 14.0.2 (set by /Users/davidlucey/.jenv/version)
  oracle64-1.6.0.65
  oracle64-1.8.0.261
  oracle64-14.0.2</code></pre>
<p>In our first pass, we didn’t understand the different paths, but it doesn’t matter anymore. Just copy/paste the name and put in in the following command and the problem is solved.</p>
<pre class="bash"><code>jenv global 1.8</code></pre>
<p>And we are good to go, plus we can easily switch back when we are done. It is hard to understate how grateful we are to people who built <code>jenv</code> and <code>brew</code>.</p>
<pre class="bash"><code>jenv version</code></pre>
<pre><code>1.8 (set by /Users/davidlucey/.jenv/version)</code></pre>
</div>
<div id="setting-up-apache-drill" class="section level1">
<h1>Setting up Apache Drill</h1>
<p>The latest version (December 2019) can be downloaded from <a href="http://apache.mirrors.hoobly.com/drill/drill-1.17.0/apache-drill-1.17.0.tar.gz">here</a>, but note with the sale of MapR to Hewlett Packard last year, the project is reported to have been “orphaned”. We took the download and install route, though we subsequently found that using <code>brew install apache-drill</code> might have avoided some of the questions we now have about symlinking (see Zookeeper section below). <a href="http://why-not-learn-something.blogspot.com/2016/01/apache-drill-quick-setup-and-examples.html">Apache Drill : Quick Setup and Examples</a> gives step-by-step instructions which might have helped if we had it while installing, but currently have Drill installed in <code>/usr/local/apache-drill-1.1.7.0/</code> (shown below) though the {sergeant} manual directs to install in the <code>drill/</code> folder.</p>
<pre class="bash"><code>ls /usr/local/apache-drill-1.17.0/bin</code></pre>
<pre><code>auto-setup.sh
drill-am.sh
drill-conf
drill-config.sh
drill-embedded
drill-embedded.bat
drill-localhost
drill-on-yarn.sh
drillbit.sh
hadoop-excludes.txt
runbit
sqlline
sqlline.bat
submit_plan
yarn-drillbit.sh</code></pre>
<p>Here there are a few options for running Drill. Running <code>bin/drill-embedded</code> from this this folder, a SQL engine comes up, and queries can be run straight from the command line from a basic UI. We wanted to query from RStudio, so we had another step or two. First, we had to configure the <code>drill-override.conf</code> file in the /<code>conf/</code> folder above. We followed Bob Rudis’ <a href="https://rud.is/books/drill-sergeant-rstats/drill-in-more-than-10-minutes.html#drill-storage-plugins">instructions</a> and named our cluster_id “drillbit1” and zk.connect to our local path as shown below. After these steps, we are able to run and show some sample queries using Drill.</p>
<pre class="bash"><code>grep &quot;^[^#;]&quot; /usr/local/apache-drill-1.17.0/conf/drill-override.conf</code></pre>
<pre><code>drill.exec: {
  cluster-id: &quot;drillbits1&quot;,
  zk.connect: &quot;localhost:2181&quot;,  
  store.json.reader.skip_invalid_records: true,
  sys.store.provider.local.path: &quot;/usr/local/apache-drill-1.17.0/conf/storage.conf&quot; 
}</code></pre>
<p>Once this was all in place, the start up to run Drill in the local environment is pretty easy just running <code>bin/drillbit.sh start</code> from in the Terminal. We are not actually running it here in RMarkdown because it froze up the chunk while Drill was running.</p>
<pre class="bash"><code># Run in Terminal not in .rmd chunk
~/usr/local/apache-drill-1.17.0/bin/drillbit.sh start</code></pre>
<p>We actually ran it separately in the background from Terminal. Below, we are able to check the status and see that drillbit is running.</p>
<pre class="bash"><code>/usr/local/apache-drill-1.17.0/bin/drillbit.sh status</code></pre>
<pre><code>/usr/local/apache-drill-1.17.0/bin/drill-config.sh: line 144: let: lineCount=: syntax error: operand expected (error token is &quot;=&quot;)
/usr/local/apache-drill-1.17.0/bin/drill-config.sh: line 144: let: lineCount=: syntax error: operand expected (error token is &quot;=&quot;)
/usr/local/apache-drill-1.17.0/bin/drill-config.sh: line 144: let: lineCount=: syntax error: operand expected (error token is &quot;=&quot;)
/usr/local/apache-drill-1.17.0/bin/drill-config.sh: line 144: let: lineCount=: syntax error: operand expected (error token is &quot;=&quot;)
drillbit is running.</code></pre>
<p>The <code>{sergeant}</code> manual also talked about allocating more memory, but we didn’t know how to do this or if it was possible on our small system. There were also other options for setting up a Drill connection, like Docker, so maybe that would help us resolve our issues. It could be that these factors are why we haven’t gotten it to work as well as we hoped.</p>
</div>
<div id="zookeeper" class="section level1">
<h1>Zookeeper</h1>
<p>There is also the option to run Drill in parallel using Zookeeper discussed in the <code>{sergeant}</code> manual. In the <em>Wiring Up Zookeeper</em> section, it says to have drill in <code>usr/local/drill/</code> for Mac, and to symlink to the full versioned <code>drill</code> to make it easier to upgrade, but it was vague about this. We noticed that we have a separate folder (<code>~/drill/</code>) in our home directory which has a file <code>udf/</code> file from the installation, which we understand pertains to “user defined functions” (a subject touched on in Recipe 11 of the <code>{sergeant}</code> manual). We weren’t sure exactly which folder was referred to and reading on Stack Overflow, but we were about three steps away from understanding how this all fit together, so our configuration may not be optimal. When we used Zookeeper with the ODBC connection in parallel instead of “Direct to”Drillbit", if anything, we got slower query times as we will discuss below.</p>
</div>
<div id="configuring-the-drill-path-storage-plug-in" class="section level1">
<h1>Configuring the Drill Path Storage Plug-in</h1>
<p>Drill is connected to data sources via <a href="https://drill.apache.org/docs/storage-plugin-registration/">storage plug-ins</a>. The <code>{sergeant}</code> manual mentioned the Drill Web UI passing, but we didn’t realize at first that pulling up <code>localhost:8047</code> in our browser was an important component for profiling queries. We will show a few of the pages below.</p>
<pre class="bash"><code># Run in terminal not .rmd chunk
/usr/local/apache-drill-1.17.0/bin/drill-localhost</code></pre>
<p>In his Yelp blog post, Bob Rudis used “root.dfs” as the path to the Yelp tables. At first, we didn’t understand what this referred to, but it is used as the path to the root of the file system where the data is stored as configured in the storage plug-ins. The “Storage” page of the Drill Web App is in <em>Drill Web App Plug-Ins</em> below. Both his and the Apache documentation also refer the “cp” path to refer to example JAR data in the Drill “classpath”. In addition to the two defaults, all the plug-ins available for hive, mongo, s3, kafka, etc. are also shown below.</p>
<div class="figure">
<img src="/img/drill/drill-plugins.png" alt="" />
<p class="caption">Drill Web App Plug-Ins</p>
</div>
<p>By clicking on the “Update” button for “dfs”, it is easy to modify the “workspace”, “location” and “defaultInputFormat” with the path to the file with your data as shown in <em>Drill Web App Storage DFS Panel</em> below. In our case, we changed the name of workspace to “root”, the location to “/Volumes/davidlucey/aDL/data/yelp_dataset/” and the defaultInputFormat to “json”. All the different data types are shown further down in “formats”, which is one of the big selling points. According to <code>{sergeant}</code>, it is possible to even combine disparate source types like: json, csv, parquet and rmdbs by modifying formats when configuring “dfs”, while pointing to almost any distributed file system. Once a path is configured in the plug-in, the data in that folder is all set to be queried from RStudio.</p>
<div class="figure">
<img src="/img/drill/drill-plugin-dfs.png" alt="" />
<p class="caption">Drill Web App Storage DFS Panel</p>
</div>
</div>
<div id="connecting-to-drill-via-dplyr" class="section level1">
<h1>Connecting to Drill via <code>dplyr</code></h1>
<p>The first and most basic option to connect given by <code>{sergeant}</code> was via <code>dplyr</code> through the REST API, which was simple using <code>src_drill()</code> mapped to “localhost” port 8047. The resulting object lists the tables, including “dfs.root” workspace, which we configured in the dfs storage page above to point to the folder where we stored the Yelp JSON files. Note that there is no connection object involved with this option, and <code>src_drill()</code> doesn’t offer the option to specify much other than the host, port and user credentials.</p>
<pre class="r"><code>db &lt;- src_drill(&quot;localhost&quot;)
db</code></pre>
<pre><code>src:  DrillConnection
tbls: cp.default, dfs.default, dfs.root, dfs.tmp, information_schema, sys</code></pre>
<p>Here we have loaded the key tables with the <code>tbl()</code> similar to <a href="https://rud.is/rpubs/yelp.html">Analyzing the Yelp Academic Dataset w/Drill &amp; sergeant</a>. Note the prefix “dfs.root”, followed by the name of the file from the specified Yelp Academic data set folder surrounded by back ticks. Our understanding is that <code>{sergeant}</code> uses <code>jsonlite::fromJSON()</code> to interact with the files while using the <code>dplyr</code> <code>tbl()</code> method to connect.</p>
<details>
<p><summary>Click to see R code to set up check, yelp_biz, users &amp; review <code>tbl()</code></summary></p>
<pre class="r"><code>tic.clearlog()
tic(&quot;Loading the four key datasets with: &quot;)
check &lt;- tbl(db, &quot;dfs.root.`yelp_academic_dataset_checkin.json`&quot;)
yelp_biz &lt;-
  tbl(db, &quot;dfs.root.`yelp_academic_dataset_business.json`&quot;)
users &lt;- tbl(db, &quot;dfs.root.`yelp_academic_dataset_user.json`&quot;)
review &lt;- tbl(db, &quot;dfs.root.`yelp_academic_dataset_review.json`&quot;)
toc(log = TRUE, quiet = TRUE)
yelp_biz.txt &lt;- tic.log(format = TRUE)</code></pre>
</details>
<pre><code># Source:   table&lt;dfs.root.`yelp_academic_dataset_checkin.json`&gt; [?? x 10]
# Database: DrillConnection
   business_id         date                                                     
   &lt;chr&gt;               &lt;chr&gt;                                                    
 1 --1UhMGODdWsrMastO… 2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016-10-15 02:…
 2 --6MefnULPED_I942V… 2011-06-04 18:22:23, 2011-07-23 23:51:33, 2012-04-15 01:…
 3 --7zmmkVg-IMGaXbuV… 2014-12-29 19:25:50, 2015-01-17 01:49:14, 2015-01-24 20:…
 4 --8LPVSo5i0Oo61X01… 2016-07-08 16:43:30                                      
 5 --9QQLMTbFzLJ_oT-O… 2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010-12-09 21:…
 6 --9e1ONYQuAa-CB_Rr… 2010-02-08 05:56:47, 2010-02-15 04:47:42, 2010-02-22 03:…
 7 --DaPTJW3-tB1vP-Pf… 2012-06-03 17:46:09, 2012-08-04 16:19:52, 2012-08-04 16:…
 8 --DdmeR16TRb3LsjG0… 2012-11-02 21:26:42, 2012-11-02 22:30:43, 2012-11-02 22:…
 9 --EF5N7P70J_UYBTPy… 2018-05-25 19:52:07, 2018-09-18 16:09:44, 2019-10-18 21:…
10 --EX4rRznJrltyn-34… 2010-02-26 17:05:40, 2012-12-29 20:05:04, 2012-12-30 22:…
# … with more rows</code></pre>
<pre><code># Source:   table&lt;dfs.root.`yelp_academic_dataset_business.json`&gt; [?? x 22]
# Database: DrillConnection
   business_id name  address city  state postal_code latitude longitude stars
   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
 1 f9NumwFMBD… The … 10913 … Corn… NC    28031           35.5     -80.9   3.5
 2 Yzvjg0Sayh… Carl… 8880 E… Scot… AZ    85258           33.6    -112.    5  
 3 XNoUzKckAT… Feli… 3554 R… Mont… QC    H4C 1P4         45.5     -73.6   5  
 4 6OAZjbxqM5… Neva… 1015 S… Nort… NV    89030           36.2    -115.    2.5
 5 51M2Kk903D… USE … 4827 E… Mesa  AZ    85205           33.4    -112.    4.5
 6 cKyLV5oWZJ… Oasi… 1720 W… Gilb… AZ    85233           33.4    -112.    4.5
 7 oiAlXZPIFm… Gree… 6870 S… Las … NV    89118           36.1    -115.    3.5
 8 ScYkbYNkDg… Junc… 6910 E… Mesa  AZ    85209           33.4    -112.    5  
 9 pQeaRpvuho… The … 404 E … Cham… IL    61820           40.1     -88.2   4.5
10 EosRKXIGeS… Xtre… 700 Ki… Toro… ON    M8Z 5G3         43.6     -79.5   3  
# … with more rows, and 5 more variables: review_count &lt;dbl&gt;, is_open &lt;dbl&gt;,
#   attributes &lt;chr&gt;, categories &lt;chr&gt;, hours &lt;chr&gt;</code></pre>
<pre><code># Source:   table&lt;dfs.root.`yelp_academic_dataset_user.json`&gt; [?? x 30]
# Database: DrillConnection
   user_id name  review_count yelping_since useful funny  cool elite friends
   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  
 1 ntlvfP… Rafa…          553 2007-07-06 0…    628   225   227 &quot;&quot;    oeMvJh…
 2 FOBRPl… Mich…          564 2008-04-28 0…    790   316   400 &quot;200… ly7EnE…
 3 zZUnPe… Mart…           60 2008-08-28 2…    151   125   103 &quot;201… Uwlk0t…
 4 QaELAm… John           206 2008-09-20 0…    233   160    84 &quot;200… iog3Ny…
 5 xvu8G9… Anne           485 2008-08-09 0…   1265   400   512 &quot;200… 3W3ZMS…
 6 z5_82k… Steve          186 2007-02-27 0…    642   192   155 &quot;200… E-fXXm…
 7 ttumcu… Stua…           12 2010-05-12 1…     29     4     6 &quot;&quot;    1pKOc5…
 8 f4_MRN… Jenn…          822 2011-01-17 0…   4127  2446  2878 &quot;201… c-Dja5…
 9 UYACF3… Just…           14 2007-07-24 2…     68    21    34 &quot;&quot;    YwaKGm…
10 QG13XB… Clai…          218 2007-06-04 0…    587   372   426 &quot;200… tnfVwT…
# … with more rows, and 13 more variables: fans &lt;dbl&gt;, average_stars &lt;dbl&gt;,
#   compliment_hot &lt;dbl&gt;, compliment_more &lt;dbl&gt;, compliment_profile &lt;dbl&gt;,
#   compliment_cute &lt;dbl&gt;, compliment_list &lt;dbl&gt;, compliment_note &lt;dbl&gt;,
#   compliment_plain &lt;dbl&gt;, compliment_cool &lt;dbl&gt;, compliment_funny &lt;dbl&gt;,
#   compliment_writer &lt;dbl&gt;, compliment_photos &lt;dbl&gt;</code></pre>
<pre><code># Source:   table&lt;dfs.root.`yelp_academic_dataset_review.json`&gt; [?? x 17]
# Database: DrillConnection
   review_id   user_id   business_id stars useful funny  cool text        date  
   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; 
 1 xQY8N_XvtG… OwjRMXRC… -MhfebM0QI…     2      5     0     0 &quot;As someon… 2015-…
 2 UmFMZ8PyXZ… nIJD_7ZX… lbrU8StCq3…     1      1     1     0 &quot;I am actu… 2013-…
 3 LG2ZaYiOgp… V34qejxN… HQl28KMwrE…     5      1     0     0 &quot;I love De… 2015-…
 4 i6g_oA9Yf9… ofKDkJKX… 5JxlZaqCnk…     1      0     0     0 &quot;Dismal, l… 2011-…
 5 6TdNDKywdb… UgMW8bLE… IS4cv902yk…     4      0     0     0 &quot;Oh happy … 2017-…
 6 L2O_INwlrR… 5vD2kmE2… nlxHRv1zXG…     5      2     0     0 &quot;This is d… 2013-…
 7 ZayJ1zWyWg… aq_ZxGHi… Pthe4qk5xh…     5      1     0     0 &quot;Really go… 2015-…
 8 lpFIJYpsvD… dsd-KNYK… FNCJpSn0tL…     5      0     0     0 &quot;Awesome o… 2017-…
 9 JA-xnyHytK… P6apihD4… e_BiI4ej1C…     5      0     0     0 &quot;Most deli… 2015-…
10 z4BCgTkfNt… jOERvhmK… Ws8V970-mQ…     4      3     0     1 &quot;I have be… 2009-…
# … with more rows</code></pre>
<pre><code>[[1]]
[1] &quot;Loading the four key datasets with: : 93.973 sec elapsed&quot;</code></pre>
<p>It takes about two minutes to skim <code>yelp_biz</code>, which seems too long for ~210k rows, and definitely not worth it with the other, much larger files. <a href="https://rud.is/rpubs/yelp.html">Analyzing the Yelp Academic Dataset w/Drill &amp; sergeant</a> didn’t give the timing on its queries, but we assume it was much faster than this. The error message recommends that we <code>CAST</code> BIGINT columns to <code>VARCHAR</code> prior to working with them in <code>dplyr</code>, and suggests that we consider using R ODCBC with the MapR ODBC Driver because <code>jsonlite::fromJSON()</code> doesn’t support 64-bit integers. So, we are going to give ODBC a try in the next section and will set up a query to try to take this message into account to see if that makes a difference.</p>
<details>
<p><summary>Click to see R code to skim Yelp Business JSON</summary></p>
<pre class="r"><code>tic.clearlog()
tic(&quot;Time to skim: &quot;)
skim &lt;- skimr::skim(yelp_biz)
toc(log = TRUE, quiet = TRUE)
yelp_biz_skim.txt &lt;- tic.log(format = TRUE)</code></pre>
</details>
<pre><code>── Data Summary ────────────────────────
                           Values  
Name                       yelp_biz
Number of rows             209393  
Number of columns          14      
_______________________            
Column type frequency:             
  character                9       
  numeric                  5       
________________________           
Group variables            None    

── Variable type: character ────────────────────────────────────────────────────
  skim_variable n_missing complete_rate   min   max empty n_unique whitespace
1 business_id           0         1        22    22     0   209393          0
2 name                  0         1         0    64     1   157221          0
3 address               0         1         0   118  8679   164423          0
4 city                  0         1         0    43     2     1243          0
5 state                 0         1         2     3     0       37          0
6 postal_code           0         1         0     8   509    18605          0
7 attributes            0         1         2  1542     0    78140          0
8 categories          524         0.997     4   550     0   102494          0
9 hours                 0         1         2   170     0    57641          0

── Variable type: numeric ──────────────────────────────────────────────────────
  skim_variable n_missing complete_rate    mean      sd     p0    p25    p50
1 latitude              0             1  38.6     4.94    21.5   33.6   36.1
2 longitude             0             1 -97.4    16.7   -158.  -112.  -112. 
3 stars                 0             1   3.54    1.02     1      3      3.5
4 review_count          0             1  36.9   123.       3      4      9  
5 is_open               0             1   0.807   0.395    0      1      1  
    p75    p100 hist 
1  43.6    51.3 ▁▂▇▆▂
2 -80.0   -72.8 ▁▁▇▁▇
3   4.5     5   ▁▃▃▇▆
4  27   10129   ▇▁▁▁▁
5   1       1   ▂▁▁▁▇</code></pre>
<pre><code>[[1]]
[1] &quot;Time to skim: : 262.239 sec elapsed&quot;</code></pre>
</div>
<div id="setting-up-and-querying-drill-with-odbc" class="section level1">
<h1>Setting up and Querying Drill with ODBC</h1>
<p>First we had to download and install the MapR Drill ODBC Driver, which wasn’t difficult with the instructions <a href="https://drill.apache.org/docs/installing-the-driver-on-mac-os-x/">here</a>.</p>
<pre><code>                    name              attribute
6           ODBC Drivers MapR Drill ODBC Driver
7 MapR Drill ODBC Driver            Description
8 MapR Drill ODBC Driver                 Driver
                                           value
6                                      Installed
7                         MapR Drill ODBC Driver
8 /Library/mapr/drill/lib/libdrillodbc_sbu.dylib</code></pre>
<p>Here was our connection using ODBC. Note that “ConnectionType” is specified as “Direct to Drillbit” <a href="https://rud.is/books/drill-sergeant-rstats/wiring-up-drill-and-r-odbc-style.html">Wiring Up Drill and R ODBC Style</a>. If we were going with Zookeeper, ConnectionType should be “Zookeeper” and “ZKQuorum” “localhost:2181” instead. Since we have Zookeeper installed, we also tried this, but didn’t notice a big difference. When we run the ODBC connection below, the connection pain in RStudio shows four schemas (“c”, “d”, “i” and “s”), each having no tables.</p>
<details>
<p><summary>Click to see R code to connect via ODBC</summary></p>
<pre class="r"><code>DBI::dbConnect(
  odbc::odbc(),
  driver = &quot;MapR Drill ODBC Driver&quot;,
  Host = &quot;localhost&quot;,
  Port = &quot;31010&quot;,
  ConnectionType = &quot;Direct to Drillbit&quot;,
  AuthenticationType = &quot;No Authentication&quot;,
  ZkClusterID = &quot;drillbits1&quot;,
  ZkQuorum = &quot;&quot;
) -&gt; drill_con</code></pre>
</details>
<pre><code>&lt;OdbcConnection&gt;  Database: DRILL
  Drill Version: 00.00.0000</code></pre>
<p>After setting up the connection, the <code>{sergeant}</code> manual returned a message with the current Drill version, but ours showed a Drill version of “00.00.0000”, so that might be part of to problem. We can see that connecting to the tables with ODBC took almost twice as long as with the <code>dplyr</code> connection, so it seems like we are doing something wrong. When we tried this with Zookeeper (not shown), it took 50 seconds, while 33 seconds with “Direct to Drillbit” (below).</p>
<pre class="r"><code>tic.clearlog()
tic(&quot;Loading the four key datasets with ODBC: &quot;)
check &lt;-
  tbl(drill_con,
      sql(&quot;SELECT * FROM dfs.root.`yelp_academic_dataset_checkin.json`&quot;))
yelp_biz &lt;-
  tbl(drill_con,
      sql(
        &quot;SELECT * FROM dfs.root.`yelp_academic_dataset_business.json`&quot;
      ))
users &lt;-
  tbl(drill_con,
      sql(&quot;SELECT * FROM dfs.root.`yelp_academic_dataset_user.json`&quot;))
review &lt;-
  tbl(drill_con,
      sql(&quot;SELECT * FROM dfs.root.`yelp_academic_dataset_review.json`&quot;))
toc()</code></pre>
<pre><code>Loading the four key datasets with ODBC: : 133.307 sec elapsed</code></pre>
<pre class="r"><code>tic.clearlog()</code></pre>
<p>The <code>skim()</code> for yelp_biz took about the same amount of time, but either way, it was still way too long to be a viable alternative. Again, “Direct to Drillbit” here took 116 seconds, while 81 seconds with Zookeeper, so we are clearly doing something wrong if all the things which are supposed to speed things up are actually slowing us down.</p>
<details>
<p><summary>Click to see code to skim Yelp Business JSON with ODBC</summary></p>
<pre class="r"><code>tic(&quot;Skim yelp-biz with ODBC&quot;)
skim1 &lt;- skimr::skim(yelp_biz)
toc(log = TRUE, quiet = TRUE)
yelp_odbc_skim.txt &lt;- tic.log(format = TRUE)</code></pre>
</details>
<table>
<caption><span id="tab:print-yelp-skim">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">yelp_biz</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">209393</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">14</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">11</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">business_id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22</td>
<td align="right">22</td>
<td align="right">0</td>
<td align="right">209393</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">name</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">64</td>
<td align="right">1</td>
<td align="right">157229</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">address</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">118</td>
<td align="right">8679</td>
<td align="right">164423</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">city</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">43</td>
<td align="right">2</td>
<td align="right">1251</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">state</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">37</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">postal_code</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">509</td>
<td align="right">18605</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">review_count</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">18</td>
<td align="right">21</td>
<td align="right">0</td>
<td align="right">1320</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">is_open</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">21</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">attributes</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">1713</td>
<td align="right">0</td>
<td align="right">78140</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">categories</td>
<td align="right">524</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">550</td>
<td align="right">0</td>
<td align="right">102494</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">hours</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">206</td>
<td align="right">0</td>
<td align="right">57641</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">latitude</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">38.58</td>
<td align="right">4.94</td>
<td align="right">21.50</td>
<td align="right">33.64</td>
<td align="right">36.15</td>
<td align="right">43.61</td>
<td align="right">51.30</td>
<td align="left">▁▂▇▆▂</td>
</tr>
<tr class="even">
<td align="left">longitude</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-97.39</td>
<td align="right">16.72</td>
<td align="right">-158.03</td>
<td align="right">-112.27</td>
<td align="right">-111.74</td>
<td align="right">-79.97</td>
<td align="right">-72.81</td>
<td align="left">▁▁▇▁▇</td>
</tr>
<tr class="odd">
<td align="left">stars</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.54</td>
<td align="right">1.02</td>
<td align="right">1.00</td>
<td align="right">3.00</td>
<td align="right">3.50</td>
<td align="right">4.50</td>
<td align="right">5.00</td>
<td align="left">▁▃▃▇▆</td>
</tr>
</tbody>
</table>
<pre><code>[[1]]
[1] &quot;Skim yelp-biz with ODBC: 115.719 sec elapsed&quot;</code></pre>
</div>
<div id="drill-web-app" class="section level1">
<h1>Drill Web App</h1>
<p>As we go along making queries, everything is collected in the Web App Profiles page, as shown in <em>Drill Web App Query Profiles</em> just below. Clicking on a query here takes us to the Query and Planning page, shown in further down in <em>Drill Query and Planning Pane for Complicated SQL Query</em>. There are other dashboards which we will show below.</p>
<div class="figure">
<img src="/img/drill/drill-profiles.png" alt="" />
<p class="caption">Drill Web App Query Profiles</p>
</div>
</div>
<div id="query-profiling-with-drill" class="section level1">
<h1>Query Profiling with Drill</h1>
<p>The other interesting thing in Drill was profiling. Here is a more complicated query we experimented with with a couple of joins and some aggregations for a query which wound up taking over an hour. See that we <code>CAST</code> the integer variables in this case as we were warned above, but that also didn’t seem to make a difference.</p>
<pre class="r"><code>dq &lt;-
  odbc::dbGetQuery(drill_con, 
    &quot;SELECT b1.name
            ,CAST(b1.stars AS INT) AS stars
            ,CAST(b1.review_count AS INT) AS review_count
            ,c.reviews
      FROM (SELECT b.business_id
              ,COUNT(*) as reviews
      FROM dfs.root.`yelp_academic_dataset_user.json` AS u,
            dfs.root.`yelp_academic_dataset_review.json` AS r,
            dfs.root.`yelp_academic_dataset_business.json` AS b
      WHERE r.user_id = u.user_id
            AND b.business_id = r.business_id
      GROUP BY b.business_id, r.user_id
      HAVING COUNT(*) &gt; 10) AS c
      INNER JOIN dfs.root.`yelp_academic_dataset_business.json` b1
      ON c.business_id = b1.business_id&quot;
      )</code></pre>
<p>We are not running the query here in the blog post, but as mentioned, the timing can be seen in <em>Drill Query and Planning Pane for Complicated SQL Query</em> below the query at 1h11.</p>
<div class="figure">
<img src="/img/drill/drill-query-planning.png" alt="" />
<p class="caption">Drill Query and Planning Pane for Complicated SQL Query</p>
</div>
<p>It is amazing how much information about the query Drill gives us, shown in <em>Drill Query and Planning Pane for Complicated SQL Query</em> above Clicking on the “Edit Query” tab, and scrolling down to <em>Operator Profiles</em> (shown below), we can see that we some operators spilled to disc and that the scan operators spent more time waiting for data than processing it. We can also see that the Hash Aggregate in Fragment 1 took 13% of the query time. Further down but not shown, the Hash Joins took almost 70% of the query time, so the Hash Joins and Hash Aggregate together took 70% of the query time. Even without those bottlenecks, we probably still wouldn’t have been satisfied with the amount of time this took. Having this information, it seems like it would be possible to optimize, but we didn’t know how to do it. We have been recently learning SQL and realize that there is still a lot to learn.</p>
<div class="figure">
<img src="/img/drill/operator-profiles.png" alt="" />
<p class="caption">Drill Operator Profiles for Complicated SQL Query</p>
</div>
<p>Lastly, Drill has a nice dashboard which allowed us to for example instruct the hash joins and hash aggregations to ignore memory limits as shown in <em>Drill Web App - Options Panel</em> below. There were a lot of parameter settings available, but we were not sure how to adjust these to solve our specific problems, but would welcome any good advice or pointers.</p>
<div class="figure">
<img src="/img/drill/drill-options.png" alt="" />
<p class="caption">Drill Web App - Options Panel</p>
</div>
</div>
<div id="clean-up" class="section level1">
<h1>Clean up</h1>
<p>Shutting down when done is also easy as shown here.</p>
<pre class="bash"><code>/usr/local/apache-drill-1.17.0/bin/drillbit.sh stop</code></pre>
<p>Returning to JDK 14.0</p>
<pre class="bash"><code>jenv global 14.0.2</code></pre>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>We don’t know the status of Drill given “orphan” status, but there wasn’t much current discussion that we could find with a quick search. If these problems are fixable, we would be very grateful for feedback and promise to update this post for the benefit of others. We have read that the arrow package is a lot faster than this on similar sized data, but don’t know if it is as flexible. If there is a clearly better open source way to accomplish these objectives, such as arrow, any guidance would be much appreciated.</p>
</div>
