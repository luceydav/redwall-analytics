---
title: Handling larger than memory dataset with Arrow Table
author: David Lucey
date: "2022-11-27"
slug: setting-up-and-exploring-a-larger-than-memory-arrow-table
categories: ["R", "Code-Oriented"]
tags: ["duckdb", "arrow", "scrubcsv"]
---



<details>
<summary>
Setup
</summary>
<pre class="r"><code>library(data.table)
library(glue)
library(arrow)</code></pre>
<pre><code>## 
## Attaching package: &#39;arrow&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     timestamp</code></pre>
<pre class="r"><code>library(duckdb)</code></pre>
<pre><code>## Loading required package: DBI</code></pre>
<pre class="r"><code>library(tictoc)</code></pre>
<pre><code>## 
## Attaching package: &#39;tictoc&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:data.table&#39;:
## 
##     shift</code></pre>
<pre class="r"><code>library(ggplot2)
library(scales)
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:data.table&#39;:
## 
##     between, first, last</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(bit64)</code></pre>
<pre><code>## Loading required package: bit</code></pre>
<pre><code>## 
## Attaching package: &#39;bit&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:data.table&#39;:
## 
##     setattr</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     xor</code></pre>
<pre><code>## Attaching package bit64</code></pre>
<pre><code>## package:bit64 (c) 2011-2017 Jens Oehlschlaegel</code></pre>
<pre><code>## creators: integer64 runif64 seq :</code></pre>
<pre><code>## coercion: as.integer64 as.vector as.logical as.integer as.double as.character as.bitstring</code></pre>
<pre><code>## logical operator: ! &amp; | xor != == &lt; &lt;= &gt;= &gt;</code></pre>
<pre><code>## arithmetic operator: + - * / %/% %% ^</code></pre>
<pre><code>## math: sign abs sqrt log log2 log10</code></pre>
<pre><code>## math: floor ceiling trunc round</code></pre>
<pre><code>## querying: is.integer64 is.vector [is.atomic} [length] format print str</code></pre>
<pre><code>## values: is.na is.nan is.finite is.infinite</code></pre>
<pre><code>## aggregation: any all min max range sum prod</code></pre>
<pre><code>## cumulation: diff cummin cummax cumsum cumprod</code></pre>
<pre><code>## access: length&lt;- [ [&lt;- [[ [[&lt;-</code></pre>
<pre><code>## combine: c rep cbind rbind as.data.frame</code></pre>
<pre><code>## WARNING don&#39;t use as subscripts</code></pre>
<pre><code>## WARNING semantics differ from integer</code></pre>
<pre><code>## for more help type ?bit64</code></pre>
<pre><code>## 
## Attaching package: &#39;bit64&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     hashtab</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     :, %in%, is.double, match, order, rank</code></pre>
<pre class="r"><code>library(microbenchmark)
uscompany &lt;- &quot;~/Documents/Projects/uscompanies/data/&quot;
options(scipen = 999)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)</code></pre>
</details>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>It has been a while since <a href="https://redwallanalytics.com/2022/04/21/loading-a-large-messy-csv-using-data-table-fread-with-cli-tools/">loading Large, Messy CSV using {data.table} fread with CLI tools</a>, but there are recently developed tools, which we didn’t fully understand, to make the problem discussed in that post much more manageable. When we left off, we had used <code>scrubcsv</code> split our problematic csv into two parts, a clean 30 million-row, 28-column data set of US business addresses, and a separate problematic 1 million-rows with 22 or fewer columns. It was interesting to see what could be done on the CLI, but stacking the two significantly different subsets after splitting them seemed cumbersome considering varying column names and possibly types, when one would not fit in memory using R. The objective of this posting will be to explore how to load the two pieces of our address data into arrow tables, standardize variable types, stack and queries. One of the main themes of this blog all along is, how to become as agnostic as possible to data size, it feels like this moment may have arrived with <code>{arrow}</code> and <code>{duckdb}</code>.</p>
</div>
<div id="other-resources" class="section level1">
<h1>Other Resources</h1>
<p>We usually try to give credit to commentators who have made our learning possible, so here are a few that have made this post possible:</p>
<ul>
<li><a href="https://arrow-user2022.netlify.app/packages-and-data.html">Apache Arrow in R</a></li>
<li><a href="https://blog.djnavarro.net/#category=Apache%20Arrow">Notes from a data witch</a></li>
<li><a href="https://www.christophenicault.com/post/large_dataframe_arrow_duckdb/">Manipulate big data with Arrow &amp; DuckDB</a></li>
<li><a href="https://arrow.apache.org/cookbook/r/">Apache Arrow Cookbook</a></li>
<li><a href="https://duckdb.org">DuckDB is an in-process SQL OLAP database management system</a></li>
<li><a href="https://emilyriederer.netlify.app/post/duckdb-carolina/">Goin’ to Carolina in my mind (or on my hard drive)</a></li>
</ul>
</div>
<div id="loading-an-arrow-table" class="section level1">
<h1>Loading an <code>{arrow}</code> Table</h1>
<p>First, we load the larger 30 million row clean data set, which has 28 columns with <code>{arrow}</code>’s <code>read_csv_arrow()</code>, setting as_data_frame equal FALSE. In the previous post, we were using a 2015 iMac with 8G of RAM, and for this, we will use our new MacBook Air m1 with 16G of RAM. When we ran the same function on the older iMac taking the data as data.frame, this took almost 10 minutes, but on the new machine, it took about 1 minute (so the time savings here with the new machine are about 90%). We also tried to load this as a data.frame with <code>{data.table}</code> <code>fread()</code>, but gave up after about 15 minutes. Here, we will actually choose the option to take the input as an <code>{arrow}</code> table, and load time takes about 45 seconds. In the context of a 7G data set and this machine, <code>{data.table}</code> doesn’t seem like a viable solution compared to <code>{arrow}</code>, much as we love it.</p>
<pre class="r"><code>tic()
cleaned_arrow &lt;- 
  arrow::read_csv_arrow(paste0(uscompany, &quot;scrubbed.csv&quot;), as_data_frame = FALSE)
cleaned_arrow</code></pre>
<pre><code>## Table
## 30266526 rows x 28 columns
## $COMPANY_NAME &lt;string&gt;
## $SIC_CODE &lt;int64&gt;
## $SIC_DESCRIPTION &lt;string&gt;
## $ADDRESS &lt;string&gt;
## $CITY &lt;string&gt;
## $STATE &lt;string&gt;
## $ZIP &lt;int64&gt;
## $COUNTY &lt;string&gt;
## $PHONE &lt;int64&gt;
## $FAX_NUMBER &lt;string&gt;
## $WEBSITE &lt;string&gt;
## $LATITUDE &lt;string&gt;
## $LONGITUDE &lt;string&gt;
## $TOTAL_EMPLOYEES &lt;int64&gt;
## $EMPLOYEE_RANGE &lt;string&gt;
## $SALES_VOLUME &lt;double&gt;
## $SALES_VOLUME_RANGE &lt;string&gt;
## $CONTACT_FIRSTNAME &lt;string&gt;
## $CONTACT_LASTNAME &lt;string&gt;
## $CONTACT_FULLNAME &lt;string&gt;
## $CONTACT_GENDER &lt;string&gt;
## $CONTACT_TITLE &lt;string&gt;
## $CONTACT2_FIRSTNAME &lt;string&gt;
## $CONTACT2_LASTNAME &lt;string&gt;
## $CONTACT2_TITLE &lt;string&gt;
## $CONTACT2_GENDER &lt;string&gt;
## $NAICS_NUMBER &lt;int64&gt;
## $INDUSTRY &lt;string&gt;</code></pre>
<pre class="r"><code>toc()</code></pre>
<pre><code>## 48.321 sec elapsed</code></pre>
<p>As an <code>{arrow}</code> table, the 7G of data takes up only 283kB of memory to be accessed via <code>{dplyr}</code> in RStudio</p>
<pre class="r"><code>lobstr::obj_size(cleaned_arrow)</code></pre>
<pre><code>## 283.62 kB</code></pre>
</div>
<div id="loading-the-bad-data" class="section level1">
<h1>Loading the Bad Data</h1>
<p>Next, we will attempt to also load the bad data into an arrow table, but will be unsuccessful, because there are additional troublesome rows even after separating most of the good rows out, because some of the rows only have 19, while most have 22 columns.</p>
<pre class="r"><code>bad_csv &lt;- 
  try(arrow::read_csv_arrow(paste0(uscompany, &quot;bad_scrub_data.csv&quot;)))</code></pre>
<pre><code>## Error in (function (file, delim = &quot;,&quot;, quote = &quot;\&quot;&quot;, escape_double = TRUE,  : 
##   Invalid: CSV parse error: Expected 22 columns, got 19: &quot;Ahlstrom, aaron&quot;,6282,FINANCIAL ADVISORY SERVICES,753 ameriprise financial ctr,Minneapolis,MN,5 ...</code></pre>
<p>So, we will back to <code>scrubscv</code> and <code>{data.table}</code> <code>fread()</code>, piping the command line function into <code>fread()</code>. We can see that the 1 million rows with the retained 22 columns takes about a second to load, with an additional 5,598 rows thrown out by <code>scrubcsv</code> for having non-standard columns. For this amount of data, it is hard to think of a better option!</p>
<pre class="r"><code>cmd &lt;- glue(&quot;scrubcsv { paste0(uscompany, &#39;bad_scrub_data.csv&#39;) }&quot;)
bad_data &lt;- fread(cmd= cmd)</code></pre>
</div>
<div id="preparing-bad_data-to-be-bound-to-the-larger-clean-dataset" class="section level1">
<h1>Preparing bad_data to be bound to the larger clean dataset</h1>
<p>As mentioned previously, <code>scrubcsv</code> takes bad rows (ie: less than 22 columns) and throws them out. It also doesn’t import column names with the discarded rows, so these have to be added after the fact, and since the table scan will be a significantly different amount of rows, it seems reasonable to expect some types may vary. We threw out the names of columns which were not in bad_data, and then mapped the appropriate column names from cleaned_arrow to bad_data.</p>
<pre class="r"><code>table_names &lt;- names(cleaned_arrow)
names(bad_data) &lt;-
  table_names[
    !table_names %in% c(
      &quot;CONTACT2_FIRSTNAME&quot;,
      &quot;CONTACT2_LASTNAME&quot;,
      &quot;CONTACT2_TITLE&quot;,
      &quot;CONTACT2_GENDER&quot;,
      &quot;NAICS_NUMBER&quot;,
      &quot;INDUSTRY&quot;
    )]</code></pre>
</div>
<div id="stacking-the-tables" class="section level1">
<h1>Stacking the Tables</h1>
<p>For next part, we had quite a few false starts before we figured it out. <code>{arrow}</code> allows to bind rows, but obviously column names have to match, and columns must have the same data types. If we wanted to be more precise, we might change some of the column types {arrow} chose for cleaned_arrow. For example, {arrow} chose Utf8 for the LON/LAT columns, which wasn’t what we expected. It is possible to convert data types in arrow tables, but it was not nearly as clean and straightforward as doing it in the R data.table before converting to the <code>{arrow}</code> table. After a little study, we learned that by mapping R integer types to <code>{arrow}</code> int64, character to Utf8 and numeric to double, everything worked.</p>
<pre class="r"><code># Convert integers, character and numeric to align with arrow types in cleaned_arrow
convert_ints &lt;- 
  c(&quot;SIC_CODE&quot;, &quot;ZIP&quot;, &quot;TOTAL_EMPLOYEES&quot;)
bad_data[, (convert_ints) := lapply(.SD, bit64::as.integer64), .SDcols = convert_ints]
convert_utf8 &lt;-
  c(&quot;CONTACT_FULLNAME&quot;, &quot;CONTACT_GENDER&quot;, &quot;LONGITUDE&quot;, &quot;LATITUDE&quot;)
bad_data[, (convert_utf8) := lapply(.SD, as.character), .SDcols = convert_utf8]
bad_data[, SALES_VOLUME := as.numeric(SALES_VOLUME)]</code></pre>
<p>To convert from <code>{data.table}</code> to <code>{arrow}</code> takes only an instant, so again timing is not shown.</p>
<pre class="r"><code># Convert to arrow table
bad_data_arrow &lt;- 
  arrow::as_arrow_table(bad_data)
bad_data_arrow</code></pre>
<pre><code>## Table
## 1070764 rows x 22 columns
## $COMPANY_NAME &lt;string&gt;
## $SIC_CODE &lt;int64&gt;
## $SIC_DESCRIPTION &lt;string&gt;
## $ADDRESS &lt;string&gt;
## $CITY &lt;string&gt;
## $STATE &lt;string&gt;
## $ZIP &lt;int64&gt;
## $COUNTY &lt;string&gt;
## $PHONE &lt;int64&gt;
## $FAX_NUMBER &lt;string&gt;
## $WEBSITE &lt;string&gt;
## $LATITUDE &lt;string&gt;
## $LONGITUDE &lt;string&gt;
## $TOTAL_EMPLOYEES &lt;int64&gt;
## $EMPLOYEE_RANGE &lt;string&gt;
## $SALES_VOLUME &lt;double&gt;
## $SALES_VOLUME_RANGE &lt;string&gt;
## $CONTACT_FIRSTNAME &lt;string&gt;
## $CONTACT_LASTNAME &lt;string&gt;
## $CONTACT_FULLNAME &lt;string&gt;
## $CONTACT_GENDER &lt;string&gt;
## $CONTACT_TITLE &lt;string&gt;
## 
## See $metadata for additional Schema metadata</code></pre>
<p>At first, we thought we might have to add the 6 missing columns and set the column order of bad_data_arrow to match those in cleaned_arrow, but it seems to work without adjusting column order or instructions to fill empty rows. Stacking the two data sets only takes an instant (so we are again not showing timing), and gives almost the full 31 million row data set we originally set out to load.</p>
<pre class="r"><code># Clean up and bind arrow tables
full_data &lt;- arrow::concat_tables(cleaned_arrow, bad_data_arrow)
full_data</code></pre>
<pre><code>## Table
## 31337290 rows x 28 columns
## $COMPANY_NAME &lt;string&gt;
## $SIC_CODE &lt;int64&gt;
## $SIC_DESCRIPTION &lt;string&gt;
## $ADDRESS &lt;string&gt;
## $CITY &lt;string&gt;
## $STATE &lt;string&gt;
## $ZIP &lt;int64&gt;
## $COUNTY &lt;string&gt;
## $PHONE &lt;int64&gt;
## $FAX_NUMBER &lt;string&gt;
## $WEBSITE &lt;string&gt;
## $LATITUDE &lt;string&gt;
## $LONGITUDE &lt;string&gt;
## $TOTAL_EMPLOYEES &lt;int64&gt;
## $EMPLOYEE_RANGE &lt;string&gt;
## $SALES_VOLUME &lt;double&gt;
## $SALES_VOLUME_RANGE &lt;string&gt;
## $CONTACT_FIRSTNAME &lt;string&gt;
## $CONTACT_LASTNAME &lt;string&gt;
## $CONTACT_FULLNAME &lt;string&gt;
## $CONTACT_GENDER &lt;string&gt;
## $CONTACT_TITLE &lt;string&gt;
## $CONTACT2_FIRSTNAME &lt;string&gt;
## $CONTACT2_LASTNAME &lt;string&gt;
## $CONTACT2_TITLE &lt;string&gt;
## $CONTACT2_GENDER &lt;string&gt;
## $NAICS_NUMBER &lt;int64&gt;
## $INDUSTRY &lt;string&gt;</code></pre>
</div>
<div id="test-query" class="section level1">
<h1>Test Query</h1>
<p>It is amazing to get around the memory problems so easily with <code>{arrow}</code>, but it doesn’t take long to then become greedy to for efficient data manipulation. To put it through the paces, we set up a test query to filter unique COMPANY_NAME on an aggregating variable (STATE or SIC_DESCRIPTION), count the number of occurrences, arrange in descending count order, filter the top 10 values (see hidden code below) and collect back into an R data.frame. STATE has only 51, but SIC_DESCRIPTION has 8,665 distinct values so should be a bigger lift to aggregate. We had heard by simply plugging in <code>{duckdb}</code> <code>to_duckdb()</code> into our <code>{dplyr}</code> chain, we might improve the performance of the query, so have included an option for that in our benchmark examples below. Below we show the query run once with just the <code>{arrow}</code> table (at around 10 seconds), which is substantially slower than the average of the same query, once we run it 100 times in our benchmarks.</p>
<details>
<summary>
See code
</summary>
<pre class="r"><code># Sample duckdb/arrow query function
test_agg &lt;- function(data, agg_var, duck = FALSE) {
  
  if ( isTRUE(duck) ) {
    data &lt;- data |&gt; to_duckdb()
  }
  
  data |&gt;
    select( {{agg_var}}, COMPANY_NAME) |&gt;
    group_by({{agg_var}}) |&gt;
    distinct(COMPANY_NAME) |&gt;
    ungroup() %&gt;%
    group_by({{agg_var}}) |&gt;
    summarize(n = n()) |&gt;
    ungroup() |&gt;
    arrange(desc(n)) |&gt;
    head(10) |&gt;
    collect()
}</code></pre>
</details>
<pre class="r"><code># Test run of query function
tic()
test_agg(full_data, agg_var=STATE)</code></pre>
<pre><code>## # A tibble: 10 × 2
##    STATE       n
##    &lt;chr&gt;   &lt;int&gt;
##  1 CA    2815269
##  2 FL    2022421
##  3 TX    1949658
##  4 NY    1498688
##  5 PA     917881
##  6 IL     914293
##  7 MI     755170
##  8 NC     732892
##  9 NJ     730344
## 10 VA     617868</code></pre>
<pre class="r"><code>toc()</code></pre>
<pre><code>## 10.285 sec elapsed</code></pre>
</div>
<div id="query-evaluation" class="section level1">
<h1>Query evaluation</h1>
<p>So here, we benchmark four queries, aggregating on STATE and SIC_DESCRIPTION with and without duckdb. The big surprise here was how big an impact <code>{duckdb}</code> had with so little effort, reducing query time by 55-60%. It also seemed to make the query run with much less variability, but will leave that to the experts to explain.</p>
<details>
<summary>
See code
</summary>
<pre class="r"><code># Microbenchmark on 100 iterations
mbm &lt;- microbenchmark::microbenchmark(
  &quot;state&quot; = test_agg(full_data, agg_var = STATE),
  &quot;sic&quot; = test_agg(full_data, agg_var = SIC_DESCRIPTION),
  &quot;state_duck&quot; = test_agg(full_data, agg_var = STATE, duck=TRUE),
  &quot;sic_duck&quot; = test_agg(full_data, agg_var = SIC_DESCRIPTION, duck=TRUE)
)
mbm</code></pre>
<pre><code>## Unit: seconds
##        expr      min       lq     mean   median       uq       max neval  cld
##       state 6.121328 6.301117 6.502608 6.440948 6.669132  7.317626   100   c 
##         sic 8.142125 8.547280 9.078264 8.827122 9.418389 10.958355   100    d
##  state_duck 2.640490 2.793469 3.070989 2.886876 3.121399  4.777160   100 a   
##    sic_duck 3.244759 3.459508 4.102692 3.673626 4.748710  6.992540   100  b</code></pre>
</details>
<pre><code>## Coordinate system already present. Adding new coordinate system, which will
## replace the existing one.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mbm-autoplot-1.png" width="672" /></p>
<p>When we were playing around, it seemed like the first time we ran a query was slower than after a few times. It seems like there might be a cost to moving over to duckdb, but we didn’t know how that would work. Looking at the time series of the queries, it looks like the first query was often slower, and then have big spikes in volatility after a while. Possibly not surprising, the larger group aggregation (SIC_DESCRIPTION) was more volatile, but it seems clear that <code>{duckdb}</code> makes query time more consistent.</p>
<details>
<summary>
See code
</summary>
<pre class="r"><code># Over time
mbm1 &lt;- as.data.table(mbm)
mbm1[, trial := rowidv(mbm1, cols=&quot;expr&quot;)]
p &lt;- ggplot2::ggplot(mbm1,
  aes(
    x = trial,
    y = time,
    group = factor(expr),
    color = factor(expr)
  )) +
  geom_line() +
  scale_y_continuous(labels = scales::label_number(scale = 1e-9)) +
  labs(x = &quot;Trial&quot;,
       y = &quot;Time [seconds]&quot;)</code></pre>
</details>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plot-time-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Based on this analysis, <code>{arrow}</code> offers a big jump in flexibility about the kind of analysis which can be conducted, seamlessly with the same work flow, from a small machine. The ease with which one line of <code>{duckdb}</code> code reduced query time, also seems to justify all the raving on Twitter about <code>{duckdb}</code>, but possibly more excitement about <code>{arrow}</code> and the way it dovetails with everything RStudio has already created may warranted. We haven’t shown here, but also saved the data as parquet, and ran queries against it in the <code>{duckdb}</code> CLI with <code>read_parquet()</code>, and got the sense that responses were even faster (despite whatever ingestion time was needed), but maybe that may be for a future post. We cannot express enough gratitude to RStudio, and to all the people who have developed <code>{arrow}</code> and <code>{duckdb}</code> at this breakneck speed</p>
</div>
