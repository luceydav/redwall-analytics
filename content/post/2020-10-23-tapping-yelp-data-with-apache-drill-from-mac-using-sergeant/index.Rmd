---
title: Tapping Yelp data with Apache Drill from Mac using {sergeant}
author: David Lucey
date: '2020-10-23'
slug: tapping-yelp-data-with-apache-drill-from-mac-using-sergeant
categories:
  ["R", "Code-Oriented", "SQL"]
tags: ["Apache Drill", "Java"]
---


```{r 'set-up', message=FALSE, warning=FALSE, include=FALSE}

# Libraries
packages <- 
  c("tidyverse",
    "sergeant",
    "tictoc"
    )

if (length(setdiff(packages,rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

invisible(lapply(packages, library, character.only = TRUE))

knitr::opts_chunk$set(
  comment = NA,
  fig.width = 12,
  fig.height = 8,
  out.width = '100%'
)
```


![](/2020-10-23-tapping-yelp-data-with-apache-drill-from-mac-using-sergeant/index_files/Screen Shot 2020-10-27 at 2.42.02 PM.png)


# Introduction

At Redwall, we have been in nonstop exploration of new data sets over the last couple of years. As our data grows and the targets of interest get bigger, we have been finding the old method of loading csv's from disc, and working on the full set in memory is becoming less optimal. We thought we would try Apache Drill via the `{sergeant}` package created by Bob Rudis, a prolific R developer. Apache Drill seems amazing because it would allow us to be agnostic as to data source and type. Usually, we write blog posts to show off things we have learned which are actually working. The punchline in this case though, is that we were not able to get where we hoped so far with Drill. We will chronicle what we have done so far, and where we are still falling short. 

Recall in [Finding the Dimensions of `secdatabase.com` from 2010-2020 - Part 2](https://redwallanalytics.com/2020/10/12/finding-the-dimensions-of-secdatabase-com-from-2010-2020-part-2/), we were able to query a data set which was over 20GB with the AWS Athena query service with pretty much instant response. With Apache Drill on one node on our 2005 Apple iMac with 8GB of RAM, queries with a couple of joins and some aggregation were taking at least 30 minutes, but usually much longer on a much smaller data set (if they didn't crash our computer altogether). This could well be our machine, something we did wrong in configuring, poor query management or all of the above. We are writing this post in hopes of a response from experts, as well as to help others who might be trying to understand how to use Java, Drill or even the command line from RStudio. We promise to update the post with any feedback, so that it provides a pathway to others seeking to do the same.


# Yelp Academic Data Set

We were hoping to avoid downloading it, but then we found Bob Rudis' excellent [Analyzing the Yelp Academic Dataset w/Drill & sergeant](https://rud.is/rpubs/yelp.html) blog post and became intrigued by the possibility of having a connection which was so flexible about storage and data formats, and here we are. The Yelp Academic data set is about 10GB in size and took us over an hour to download, and are summarized in the image from the web page above. We hoped that we might be able to use it to explore the death rate of businesses in areas with differing COVID-19 mask and other non pharmaceutical interventions. Unfortunately, this is not possible at the moment, because it only runs through the end of last year. The files are all in JSON format, and were one of the original examples given on the Apache Drill website, and also, with the {sergeant} package. The business file is the smallest, and the reviews are by far the largest. Users visit businesses and give reviews, check-ins or tips, so the two main identifiers which tie the tables together are business_id and the user_id.


```{r 'yelp-dataset', echo=FALSE, message=FALSE, warning=FALSE}
d <-
  file.info(
    list.files(
      path = "/Volumes/davidlucey/aDL/data/yelp_dataset/",
      pattern = ".json",
      full.names = TRUE
    )
  )[c(1, 2)]
data.frame(file = stringr::str_extract(rownames(d), "yelp.*"), size = d$size)
```


# Background on Drill

To quote from this guide: [Apache Drill - Quick Guide](https://www.tutorialspoint.com/apache_drill/apache_drill_quick_guide.htm)..

*Apache Drill is a low latency schema-free query engine for big data. Drill uses a JSON document model internally which allows it to query data of any structure. Drill works with a variety of non-relational data stores, including Hadoop, NoSQL databases (MongoDB, HBase) and cloud storage like Amazon S3, Azure Blob Storage, etc. Users can query the data using a standard SQL and BI Tools, which doesnâ€™t require to create and manage schemas.*

If this could work, it feels like we could fire it up and use it in just about any source or data type. In this post, we are just going to use with a single node as we are only working with one small computer, but it looks like it should be easy to add additional nodes to speed things up.

# Sergeant

As usual, none of this would have been possible without an amazing package created and shared by a real developer, often in their free time. In this case, we relied on Bob Rudis' Drill `{sergeant}` package, blog posts and bookdown manual [Drill in More than 10 Minutes](https://rud.is/books/drill-sergeant-rstats/drill-in-more-than-10-minutes.html). He explains that he set up the interface because he saw Drill as a streamlined alternative to SPARK for those not needing the ML components (ie: just needing to query large data sources of disparate types like json, csv, parquet  and rdbms) The package allows to connect to Drill via `dplyr` interface with the `src_drill()` function, and also the REST API with `drill_connection()`. Before using `{sergeant}` though, Java, Drill and Zookeeper must be installed.

# Java

Drill requires Oracle JDK 1.8, which is several generations earlier than the version we currently have installed on our Mac. In our first year or two, we tangled with Java because we really wanted to use `{tabulizer}` to extract tables from pdfs. We burned a lot of time trying to understand the versions and how to install and point to them on Stack Overflow. Just last week, we saw a post looking for advice on loading the `{xlsx}` package, which depends on Java, as well. One of the magical discoveries we made was [Java Environment](https://www.jenv.be). Go to [Java SE Development Kit 8 Downloads](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html), choose the latest Mac Version of 1.8, and install the .dmg. Then on a Mac, `brew install jenv`, and it is off to the races. Here we show the Java versions on our machine from the Terminal. 


```{bash}
jenv versions
```

In our first pass, we didn't understand the different paths, but it doesn't matter anymore. Just copy/paste the name and put in in the following command and the problem is solved. 

```{bash 'set-jenv'}
jenv global 1.8
```

And we are good to go, plus we can easily switch back when we are done. It is hard to understate how grateful we are to people who built `jenv` and `brew`. 

```{bash 'jenv-version'}
jenv version
```


# Setting up Apache Drill

The latest version (December 2019) can be downloaded from [here](http://apache.mirrors.hoobly.com/drill/drill-1.17.0/apache-drill-1.17.0.tar.gz), but note with the sale of MapR to HPE last year, the project is reported to been "orphaned". We took the download and install route, though we subsequently found that using `brew install apache-drill` might have avoided some of the questions we now have. We currently have it installed at `/usr/local/apache-drill-1.1.7.0/` (shown below). The `{sergeant}` manual says to have drill in `usr/local/drill/`, and to symlink to the full versioned `drill` to make it easier to upgrade. We noticed that we have a separate folder (`~/drill/`) in our home directory which has a file `udf/` file from the installation, which we understand pertains to "user defined functions". We weren't sure exactly which folder was referred to and reading on Stack Overflow, but we were about three steps away from understanding how this all worked. The `{sergeant}` manual also talked about allocating more memory, but we didn't know how to do this or if it was possible on our small system. There were also other options for setting up a Drill connection, like Docker, so maybe that would help us resolve our issues. It could be that these factors are why we haven't gotten it to work as well as we hoped. 

```{bash 'drill-dir'}
ls /usr/local/apache-drill-1.17.0/bin
```

Here there are a few options. Running `bin/drill-embedded` from this this folder, a SQL engine comes up, and queries can be run straight from the command line from a basic UI. We wanted to query from RStudio, so we had another step or two. First, we had to configure the `drill-override.conf` file in the /`conf/` folder above. We followed Bob Rudis' [instructions](https://rud.is/books/drill-sergeant-rstats/drill-in-more-than-10-minutes.html#drill-storage-plugins) and named our cluster_id "drillbit1" and zk.connect to our local path as shown below. After these steps, we are able to run and show some sample queries using Drill.


```{bash 'drill-override'}
cat /usr/local/apache-drill-1.17.0/conf/drill-override.conf
```

Once this was all in place, the start up to run Drill in the local environment is pretty easy just running `bin/drillbit.sh start` from in the Terminal. We are not actually running it here in RMarkdown because it froze up the chunk while Drill was running. 


```{bash 'start-drillbit', echo=TRUE, eval=FALSE}
# Run in Terminal not in .rmd chunk
~/usr/local/apache-drill-1.17.0/bin/drillbit.sh start
```

We actually ran it separately in the background from Terminal. Below, we are able to check the status and see that drillbit is running.

```{bash 'status-drillbit', echo=TRUE}
/usr/local/apache-drill-1.17.0/bin/drillbit.sh status
```

# Zookeeper

There is another option to run Drill in parallel using Zookeeper. We did install Zookeeper according to the `{sergeant}` instructions, but when we used this with an ODBC connection, did not notice any difference when we pointed to it in the `/conf/` file. This may be where we fell short. 

# Configuring the Drill Path Storage Plug-in

Drill is connected to data sources via [storage plug-ins](https://drill.apache.org/docs/storage-plugin-registration/). The `{sergeant}` manual mentioned the Drill Web UI passing, but we didn't realize at first that pulling up `localhost:8047` in our browser was an important component for profiling queries. We will show a few of the pages below. 


```{bash 'launch-web-app-on-localhost', echo=TRUE, eval=FALSE}
# Run in terminal not .rmd chunk
/usr/local/apache-drill-1.17.0/bin/drill-localhost
```

In his Yelp blog post, Bob Rudis used "root.dfs" as the path to tables. At first, we didn't understand what this referred to, but it is used as the path to the root of the file system where the data is stored as configured in the storage plug-ins. The "Storage" page of the Drill Web App is shown below. Both his and the Apache documentation also refer the "cp" path to refer to example JAR data in the Drill "classpath". In addition to the two defaults, all the plug-ins available for hive, mongo, s3, kafka, etc. are also shown below. 

```{r 'storage', message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
knitr::include_url('http://localhost:8047/storage')
```

By clicking on the "Update" button for "dfs", it is easy to modify the "workspace", "location" and "defaultInputFormat" with the path to the file with your data. In our case, we changed the name of workspace to "root", the location to "/Volumes/davidlucey/aDL/data/yelp_dataset/" and the defaultInputFormat to "json". All the different data types are shown further down in "formats", which is one of the big selling points. According to `{sergeant}`, it is possible to even combine disparate source types like: json, csv, parquet and rmdbs by modifying formats when configuring "dfs", while pointing to almost any distributed file system. Once a path is configured in the plug-in, the data in that folder is all set to be queried from RStudio.

```{r message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
knitr::include_url('http://localhost:8047/storage/dfs')
```


# Connecting to Drill via `dplyr`

The first and most basic option to connect given by `{sergeant}` was via `dplyr` through the REST API, which was simple using `src_drill()` mapped to "localhost" port 8047. The resulting object lists the tables, including "dfs.root" workspace, which we configured in the dfs storage page above to point to the folder where we stored the Yelp JSON files. Note that there is no connection object involved with this option, and `src_drill()` doesn't offer the option to specify much other than the host, port and user credentials.


```{r 'src-drill'}
db <- src_drill("localhost")
db
```

Here we have loaded the key tables with the `tbl()` similar to [Analyzing the Yelp Academic Dataset w/Drill & sergeant](https://rud.is/rpubs/yelp.html). Note the prefix "dfs.root", followed by the name of the file from the specified Yelp Academic data set folder surrounded by back ticks. Our understanding is that `{sergeant}` uses `jsonlite::fromJSON()` to interact with the files while using the `dplyr` `tbl()` method to connect.


```{r 'load-dplyr-tables', cache=TRUE}
tic("Loading the four key datasets with: ")
check <- tbl(db, "dfs.root.`yelp_academic_dataset_checkin.json`")
yelp_biz <-
  tbl(db, "dfs.root.`yelp_academic_dataset_business.json`")
users <- tbl(db, "dfs.root.`yelp_academic_dataset_user.json`")
review <- tbl(db, "dfs.root.`yelp_academic_dataset_review.json`")
toc()
```


It takes about two minutes to skim `yelp_biz`, which seems too long for ~210k rows, and definitely not worth it with the other, much larger files. [Analyzing the Yelp Academic Dataset w/Drill & sergeant](https://rud.is/rpubs/yelp.html) didn't give the timing on its queries, but we assume it was much faster than this. The error message recommends that we `CAST` BIGINT columns to `VARCHAR` prior to working with them in `dplyr`, and suggests that we consider using R ODCBC with the MapR ODBC Driver because `jsonlite::fromJSON()` doesn't support 64-bit integers. So, we are going to give ODBC a try in the next section and will set up a query to try to take this message into account to see if that makes a difference.

```{r 'skim-yelp-biz', echo=TRUE, cache=TRUE}
tic("Time to skim: ")
skimr::skim(yelp_biz)
toc()
```


```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
knitr::include_url('http://localhost:8047/profiles')
```

# Setting up and Querying Drill with ODBC

First we had to download and install the MapR Drill ODBC Driver, which wasn't difficult with the instructions [here](https://drill.apache.org/docs/installing-the-driver-on-mac-os-x/). 


```{r 'mapr-driver', echo=FALSE, message=FALSE, warning=FALSE}
d <- odbc::odbcListDrivers()
d[str_detect(d$name, "MapR") | str_detect(d$attribute, "MapR"),]
```

Here was our connection using ODBC. Note that "ConnectionType" is specified as "Direct to Drillbit" [Wiring Up Drill and R ODBC Style](https://rud.is/books/drill-sergeant-rstats/wiring-up-drill-and-r-odbc-style.html). If we were going with Zookeeper, ConnectionType should be "Zookeeper" and "ZKQuorum" "localhost:2181" instead. Since we have Zookeeper installed, we also tried this, but didn't notice a big difference.


```{r include=FALSE}
DBI::dbConnect(
  odbc::odbc(),
  driver = "MapR Drill ODBC Driver",
  Host = "localhost",
  Port = "31010",
  ConnectionType = "Direct to Drillbit",
  AuthenticationType = "No Authentication",
  ZkClusterID = "drillbits1",
  ZkQuorum = ""
) -> drill_con

```


After setting up the connection, the `{sergeant}` manual returned a message with the current Drill version, but ours showed a Drill version of "00.00.0000", so that might be part of to problem. We can see that connecting to the tables with ODBC took almost twice as long as with the `dplyr` connection, so it seems like we are doing something wrong. 

```{r 'load-odbc-tables', cache=TRUE}
tic("Loading the four key datasets with ODBC: ")
check <-
  tbl(drill_con,
      sql("SELECT * FROM dfs.root.`yelp_academic_dataset_checkin.json`"))
yelp_biz <-
  tbl(drill_con,
      sql(
        "SELECT * FROM dfs.root.`yelp_academic_dataset_business.json`"
      ))
users <-
  tbl(drill_con,
      sql("SELECT * FROM dfs.root.`yelp_academic_dataset_user.json`"))
review <-
  tbl(drill_con,
      sql("SELECT * FROM dfs.root.`yelp_academic_dataset_review.json`"))
toc()
```


The `skim()` for yelp_biz took about the same amount of time, but either way, it was still way too long to be a viable alternative.


```{r 'skim-yelp-biz-odbc', cache=TRUE}
tic("Skim yelp-biz with ODBC")
skimr::skim(yelp_biz)
toc()
```

# Query Profiling with Drill

The other really interesting thing in Drill was profiling. Here is a more complicated query we experimented with with a couple of joins and some aggregations for a query which wound up taking over an hour. See that we `CAST` the integer variables in this case as we were warned above, but that also didn't seem to make a difference. 

```{r 'sql-query', echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
dq <-
  odbc::dbGetQuery(drill_con, 
    "SELECT b1.name
            ,CAST(b1.stars AS INT) AS stars
            ,CAST(b1.review_count AS INT) AS review_count
            ,c.reviews
      FROM (SELECT b.business_id
              ,COUNT(*) as reviews
      FROM dfs.root.`yelp_academic_dataset_user.json` AS u,
            dfs.root.`yelp_academic_dataset_review.json` AS r,
            dfs.root.`yelp_academic_dataset_business.json` AS b
      WHERE r.user_id = u.user_id
            AND b.business_id = r.business_id
      GROUP BY b.business_id, r.user_id
      HAVING COUNT(*) > 10) AS c
      INNER JOIN dfs.root.`yelp_academic_dataset_business.json` b1
      ON c.business_id = b1.business_id"
      )

```


It is amazing how much information about the query Drill gives us, shown in the frame below. Looking further down at the Operator Profiles, we can see that joining almost 10 million rows, twice, ate up over 70% of the query time. A couple of aggregations also cost almost 20%. The scan operators spent time waiting for data and some operators spilled to disc. Even without those, we still wouldn't have been satisfied with anywhere near this amount of time. Having this information, it seems like it would be possible to optimize, but we didn't know how to do it. We have been recently learning SQL and realize that there is still a lot to learn. 


```{r 'query-and-planning', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
knitr::include_url('http://localhost:8047/profiles/20746009-dec6-a077-f88a-79071cac9b86')
```


Lastly, Drill has a nice dashboard which allowed us to for example instruct the hash joins and hash aggregations to ignore memory limits. There were a lot of parameter settings available, but we were not sure how to adjust these to solve our specific problems, but would welcome any good advice or pointers.


```{r 'options', echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
knitr::include_url('http://localhost:8047/options')
```

# Clean up 

Shutting down when done is also easy as shown here.

```{bash 'turn-off-drill', echo=TRUE, eval=FALSE}
/usr/local/apache-drill-1.17.0/bin/drillbit.sh stop
```

Returning to JDK 14.0

```{bash 'fix-jenv', echo=TRUE, message=FALSE, warning=FALSE}
jenv global 14.0.2
```


# Conclusion

We don't know the status of Drill given "orphan" status, but there wasn't much current discussion that we could find with a quick search. If these problems are fixable, we would be very grateful for feedback and promise to update this post for the benefit of others. If there is a clearly better open source way to accomplish these objectives, knowing that would also be very much appreciated.




