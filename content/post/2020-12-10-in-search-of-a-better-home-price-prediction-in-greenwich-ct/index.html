---
title: In Search of a Better Home Price Prediction in Greenwich, CT - Part 1
author: David Lucey
slug: in-search-of-a-better-home-price-prediction-in-greenwich-ct-part-1
date: '2020-12-10'
categories: ["Nutmeg Project", "R", "DataExplorer"]
tags: ["connecticut", "dataviz", "realestate"]
---

<script src="index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="index_files/jquery/jquery.min.js"></script>
<link href="index_files/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="index_files/datatables-binding/datatables.js"></script>
<link href="index_files/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="index_files/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="index_files/dt-core/js/jquery.dataTables.min.js"></script>
<link href="index_files/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="index_files/crosstalk/js/crosstalk.min.js"></script>


<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="index_files/ct_real_estate_screenshot.png" alt="Average Single Family Homes in Towns of Connecticut 1999-2018" width="100%" />
<p class="caption">
Figure 1: Average Single Family Homes in Towns of Connecticut 1999-2018
</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Though losing ground in recent years, Connecticut has long had some of the highest average incomes and home prices in the country. Within the State, some towns have had significantly higher selling prices than others (shown in chart above). In this series, we will attempt to build a model to predict selling prices in the highest-priced Town of Greenwich (shown in red). In the case of Greenwich, the same house around the corner in a neighboring town, might sell for a considerably lower price because of perceived advantages, such as schools quality, tax rates and other local amenities. Even within the town, a small group of neighborhoods have average selling prices which are significantly higher than others.</p>
<p>To make it even more difficult, some preferences have been shifting in recent years as newcomers have favored smaller plots in closer proximity to shops and transportation. In addition, along with the income of town residents in aggregate and Connecticut’s financial challenges more broadly, property prices have been slowly declining since 2007. In 2017, after some commentators disparaged local selling conditions in the national press, liquidity dried up and the number of units sold fell to levels not seen since the financial crisis. The market only started picking up again in 2019, and accelerated further in 2020, as Millennial families, who had deferred normal patterns of suburban migration, finally took a serious look in hopes of riding out the NYC lock-down with a back yard.</p>
<p>In this series (of yet undetermined size), we will explore in more depth the prices of homes in Greenwich over time and in the recent period. From time to time, we have noticed that websites offering algorithms on millions of homes nationally have surprisingly large errors when “predicting” local selling prices. By its nature, real estate is an emotional purchase and market conditions fluctuate and complete and clean data is hard to come by, so there is a limit to the accuracy of such predictions, but we will explore if it is possible to do better in this series.</p>
</div>
<div id="resources-and-preparation" class="section level1">
<h1>Resources and Preparation</h1>
<p>As described in <a href="https://redwallanalytics.com/2020/07/22/using-drake-for-etl-to-build-shiny-app-for-900k-ct-real-estate-sales/">Using drake for ETL and building Shiny app for 900k CT real estate sales</a> (also the source of the graphic above), we did all of the ETL and modeling for this series using the <code>drake</code> package. Although it was bulk of the work, we are not going to discuss data preparation in this series. We tried on and off to wrangle transaction data from multiple sources for some time. Only with the addition of <code>drake</code>, we felt we were able to get clean enough data for modeling (though still not perfect). Follow the link to the full Shiny app of <a href="https://luceyda.shinyapps.io/ct_real_assess/">Connecticut Property Selling Prices vs Assessment Values over Three Revaluation Cycles</a> shown above for how a <code>{drake}</code> workflow looks.</p>
<p>As this will be the first attempt by Redwall Analytics blog site to build a model on data, there is much to learn. As usual, we looked around for other posts and books attempting to predict real estate prices, and there were not a lot. <a href="https://bradleyboehmke.github.io/HOML/">Hands-On Machine Learning with R</a> by Bradley Boehmke &amp; Brandon Greenwell was an excellent resource on the options for modeling home prices with regression, but this referred to <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Kaggle’s Ames</a> data set, which doesn’t have the time span or heterogeneity of our data.</p>
<p>In this post, we heavily use the <a href="https://boxuancui.github.io/DataExplorer/"><code>{DataExplorer}</code></a> package. In Part 3, we will also rely on the <a href="https://cran.r-project.org/web/packages/vtreat/vignettes/vtreat.html"><code>{vtreat}</code></a> package for data complexities like handling missing-ness, categorical variables with too many or rare levels and for variable selection. In Part 4, we will use the <code>{h2o}</code> <code>auto_ML()</code> functionality to explore and look for the best model.</p>
</div>
<div id="data-summary" class="section level1">
<h1>Data Summary</h1>
<p>We have gathered 17 years of Single Family home sales, and tried to filter out transactions which might not be “arms length”. For example, we edited out transactions which took more than one year to close and sold for less than 75% of original listing price assuming the normal selling process had been impaired. By convention in Connecticut, real estate is assessed at 70% of estimated market value. We filtered out homes which sold for less than 75% or greater than 180% of assessed value (ie: 52.5-126% of the Town’s best guess for tax purposes), making the assumption that there must have been special circumstances to warrant such a wide divergence.</p>
<p>In the recent slow period, it is likely that many homes came to market and were not sold on the first attempt. We are also only able to consider homes which actually sold, and are not adjusting days on the market where more than one attempt was made before selling. We also note that some of our data fields, such as <code>garage</code>, <code>taxcard</code>, <code>court</code>, <code>pool</code> and <code>condo</code>, <code>age_renov</code> and <code>age_add</code>, are only available in the last few years. Others like <code>rooms</code> is not available more recently though we have <code>beds</code> and <code>bath</code> consistently over the period.</p>
<p>Out of approximately 16,000 total property sales and 12,000 Single Family home sales during the period, we have just over 7,100 for our model building. In its 5-year revaluation conducted in 2015, we observed that the Town of Greenwich used about half of all available property sales, so it is likely that there are still transactions which can be considered non-representative, such as related parties, financial distress, and other special conditions, like homes which were purchased for demolition. After all that, it is likely that there are outlier transactions in our data set which will need further consideration if the goal is to predict the selling price of a single family home under normal circumstances.</p>
</div>
<div id="historical-and-recent-trends-since-2003" class="section level1">
<h1>Historical and Recent Trends Since 2003</h1>
<p>In recent years, the pace of sales slowed sharply, and average prices and sizes of homes sold, declined from 2007 peak levels. There has been much discussion of rising income inequality, but as we wrote in <a href="https://redwallanalytics.com/2019/01/28/irs-data-show-growth-in-number-not-income-of-highest-earners-since-2005/">IRS Data Shows Growth in Number not Income of Highest Earners since 2005</a>, income differentials between the highest and lowest earning CT groups have narrowed somewhat as conditions changed, particularly in the financial services industry, where many firms departed or closed since the GFC, and new equally high paying firms did not replace them.</p>
<p>The values of the most expensive properties have been in slow decline, while the least expensive have risen consistently since 2012, and may now exceed the previous peak levels. Even before CV-19, even with steady growth at the lower end, the average price per square foot of single family homes sold fell from over $675 to around $550. Within the overall downtrend, there have also been a change in the age and size mix, as builders replaced usually older and smaller, with larger homes on existing lots. Despite abundant new building, the average age of homes sold in the sample has risen by 5-7 years over the mid-2000s. Contributing to this, tastes have shifted from multi-acre plots in the “Back Country” to smaller lots closer to town and transportation, often in the “Coastal” area.</p>
<p>It is likely that the many owners of larger, more expensive properties far from the center, decided to hold onto them rather than sell in disadvantageous conditions. The decline in volumes shown in 2017 and 2018 and average square foot of homes sold may be partly a function of this. It most housing markets, people tend to sell in order to move to a new home, but this need can be less urgent for some in Greenwich. While the market was already picking up in 2019 and 2020, average prices may have been held back by backlog of demand to sell, which is likely to run out eventually. In short, the median and average prices probably didn’t represent the true run rate of mean valuation a couple of years ago, nor does it in 2020. The coming year will be interesting to watch.</p>
<details>
<p><summary>Click to see code generating table</summary></p>
<pre class="r"><code>#summary stats in datatable
t &lt;- 
 unique(data)[
   year(date_sold) &gt; 2000, 
   .(
     .N,
     mn_sqft = mean(sqft, na.rm = TRUE),
     mn_price = mean(price, na.rm = TRUE),
     med_price = median(price, na.rm = TRUE),
     mn_price_sqft = (sum(price, na.rm = TRUE) / sum(sqft, na.rm = TRUE)) * 1000,
     mn_age = mean(age, na.rm = TRUE)
   ), year(date_sold)][
   ][order(year)]

# Print
dt &lt;- 
  DT::datatable(
    t,
    colnames =
      c(
        &quot;Year&quot;,
        &quot;Annual Units&quot;,
        &quot;Avg. Sqft.&quot;,
        &quot;Avg. Price ($k)&quot;,
        &quot;Median Price ($k)&quot;,
        &quot;Avg Price/Sqft. ($)&quot;,
        &quot;Avg. Age&quot;
      ),
    rownames = FALSE,
    options = 
      list(
        pageLength = 18,
        scrollY = TRUE)
  ) %&gt;%
  formatRound(columns = 1,
              digits = 0) %&gt;%
  formatRound(columns = c(2:7),
              mark = &quot;,&quot;,
              digits = 0) </code></pre>
</details>
<div class="figure"><span id="fig:summary"></span>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[[2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020],[543,609,505,388,366,232,203,322,374,429,478,451,436,391,247,230,327,484],[3006.29281767956,3385.81116584565,3676.01188118812,3840.60567010309,4073.07650273224,3519.75862068966,3844.74876847291,3810.59627329193,3801.95454545455,3662.10023310023,3617.7949790795,3752.42350332594,3729.04128440367,3582.90242966752,3314.61943319838,3435.01565217391,3914.77064220184,4387.20661157025],[1688.93309023941,2194.29786699507,2489.13086336634,2504.08448453608,2749.03936338798,2323.41788793103,2217.74332019704,2244.31433850932,2131.92309358289,2208.77573659674,2085.42282635983,2302.81570066519,2312.07205963303,2070.89809462916,1884.87037651822,1921.76548695652,2173.07329357798,2394.58622727273],[1175.48,1645,1829,1932.5,2000,1865,1537.5,1700,1539.75,1631.25,1675,1775,1797.5,1625,1475,1462.5,1800,1953.5],[561.799263300982,648.086310639542,677.128078966336,652.002496384604,674.929469540753,660.107165949859,576.823988704445,588.966706927072,560.743972105538,603.14453346539,576.434772677594,613.687580472753,620.017823160883,577.994554772547,568.653631134796,559.463385775345,555.095941037238,545.811136625651],[55.8011049723757,48.8472906403941,51.5861386138614,48.5309278350515,47.5273224043716,52.5905172413793,50.743842364532,48.5931677018634,52.8877005347594,55.8927738927739,55.918410041841,58.5454545454545,58.4954128440367,57.0920716112532,64.5141700404858,58.8695652173913,58.3272171253823,56.7252066115703]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Year<\/th>\n      <th>Annual Units<\/th>\n      <th>Avg. Sqft.<\/th>\n      <th>Avg. Price ($k)<\/th>\n      <th>Median Price ($k)<\/th>\n      <th>Avg Price/Sqft. ($)<\/th>\n      <th>Avg. Age<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":18,"scrollY":true,"columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"targets":2,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"targets":3,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"targets":4,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"targets":5,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"targets":6,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"targets":0,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 0, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":[0,1,2,3,4,5,6]}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[10,18,25,50,100]}},"evals":["options.columnDefs.0.render","options.columnDefs.1.render","options.columnDefs.2.render","options.columnDefs.3.render","options.columnDefs.4.render","options.columnDefs.5.render","options.columnDefs.6.render"],"jsHooks":[]}</script>
<p class="caption">
Figure 2: Summary of Subset of Greenwich, CT Home Sale Statistics from 03-2020
</p>
</div>
</div>
<div id="assessed-values" class="section level1">
<h1>Assessed Values</h1>
<p>Key variables for our analysis are not hard to find, with assessments, available <em>ex ante</em> for model building, looking like a front-runner. The “Overall Quality” variable was one of the most important in the Kaggle Ames data, but we don’t have an equivalent measure for recent periods. Every five years, the town visits homes in order to determine the “condition” and “grade” when setting the portion of value attributable to “improvements” for tax purposes. This portion of the assessed value would ideally be our proxy for “Overall Quality”, but we only have it for the period of 2010-18. Since there are ~14,000 single family homes, and we can only remember someone from the Assessor’s office showing up once in 20 years, so this may also not be the most consistent and accurate measurement. We do however have the two components of the assessed value, land and improvements, up until the most recent sales. In Greenwich, the ratio of improvements to total assessed value is only 50% on average. In comparison, we discovered that improvements made up 85% of assessed value in one area in Florida so the size and attributes of the physical structure is likely to be less than in most places. For our analysis, we will subdivide assessed value into its land (<code>assess_land</code>) and improvement (<code>assess_improve</code>) components in hopes that the portion attributable to improvements can be a proxy for “Overall Quality” though imperfect.</p>
</div>
<div id="correlations-over-early-years" class="section level1">
<h1>Correlations over Early Years</h1>
<p>Over the 2003-2016 period shown in Figure <a href="#fig:corr-plot-all">3</a>, we can see that <code>sqft</code> is likely to be the most important variable for predicting <code>price</code>, with 88% positive correlation (shown in red), but related variables like <code>beds</code> and <code>bath</code> are also highly correlated. Similarly, <code>assess_land</code> and <code>assess_improve</code> are positively correlated, though less than <code>sqft</code>. We are focusing on prediction in this series, but several of our many important independent variables are highly correlated with each other, which would cause problems if our focus was inference. The Floor Area Ratio (<code>far</code>) is a function of <code>acre</code> and zoning (not shown here). FAR has become an increasingly important consideration over the period, as it governs the maximum size of home which can be built on a given lot. With the move towards smaller lots in the Coastal areas, builders scooped up lots perceived to be less than the best use of that land. Not surprisingly, <code>age</code> is negatively correlated (older homes are less desired). Older homes sold also have fewer bathrooms, bedrooms, square feet and even acreage shown in blue. The higher correlation of <code>price</code> with <code>bath</code> than <code>beds</code> is interesting, but this may be a function of <code>age</code>. Newer homes probably are built with more bathrooms on balance.</p>
<p>Days on market (<code>dom</code>) is only slightly correlated with <code>price</code>, but we have filtered out the really stale listings (over 365 days), so it would probably look different if we had all sales. We can also see that <code>dom</code> is negatively correlated to <code>sp_olp</code> (selling price divided by original listing price). Homes which are on the market longer sell for relatively less compared to the original listing price as prices are reduced. The Assessment Ratio (<code>AR</code>), the ratio of selling price to assessed value, should be unbiased with respect to selling prices, but we can see that the correlation is somewhat positive during this period. This suggests that homes with higher priced homes were sold for relatively more versus assessed value than would have been expected (ie: they were under-assessed). We can also see that <code>AR</code> is positively correlated to <code>sqft</code>, <code>beds</code>, <code>bath</code>, and negatively correlated to <code>age</code>. It suggests that the assessment process may have not fully accounted on the tax rolls for newer, larger homes, at least in the population which was sold, at least up until 2016.</p>
<details>
<p><summary>Click to see code generating table</summary></p>
<pre class="r"><code># Select vars
late_vars &lt;- 
  c(&quot;condo&quot;, &quot;court&quot;, &quot;pool&quot;, &quot;garage&quot;, &quot;taxcard&quot;, &quot;qtr_median&quot;)
chart_vars &lt;- 
  setdiff(vars, late_vars)

# Separate continuous variables and log transform where appropriate
cont &lt;- 
  split_columns(copy(data[between(year(date_sold), 2004, 2016), ..chart_vars]))$continuous

# Helper to filter skewed vars
skewed &lt;- function(var) abs(e1071::skewness(var, na.rm = TRUE)) &gt; 1
is_skewed &lt;- names(cont)[sapply(cont, skewed)]
is_skewed &lt;- setdiff(is_skewed, &quot;bath&quot;)
cont[, (is_skewed) := lapply(.SD, log1p), .SDcols = is_skewed]

# Corrplot of 2004-2016
p &lt;- plot_correlation(
  cont, 
  type = &quot;continuous&quot;, 
  cor_args = list(&quot;use&quot; = &quot;pairwise.complete.obs&quot;),
  title = &quot;Correlation Plot of Key Variables - 2004-2016&quot;,
  ggtheme = theme_bw()
  ) </code></pre>
<img src="index_files/figure-html/run-continous-corr-all-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:corr-plot-all"></span>
<img src="index_files/figure-html/corr-plot-all-1.png" alt="Pairwise Correlations of Key Numeric Variables from 2003-2016" width="100%" />
<p class="caption">
Figure 3: Pairwise Correlations of Key Numeric Variables from 2003-2016
</p>
</div>
</div>
<div id="correlations-since-2017" class="section level1">
<h1>Correlations Since 2017</h1>
<p>In Figure <a href="#fig:corr-plot-late">4</a> below, we isolate the years 2017-20 where we have additional variables including: <code>age_renov</code> (years since renovation), <code>age_add</code> (years since addition), <code>court</code>, <code>pool</code>, <code>garage</code> (number of garages) and <code>taxcard</code> (source of <code>sqft</code> data). There don’t seem to be many significant differences in the correlations of key variables linked to <code>price</code> from the earlier period in Figure <a href="#fig:corr-plot-all">3</a> above. One key difference though, is that the Assessor’s Office seems to have adjusted assessed values to better reflect selling prices in the 2015 revaluations. Selling prices of larger homes are now negatively correlated with Assessed Value. Not surprisingly, <code>age_renov</code> (age of renovation) and <code>age_add</code> (age of addition), which we didn’t have for the early period, are the similarly negatively correlated with <code>price</code> as above. <code>age</code> now is essentially uncorrelated to <code>AR</code>. We will see in Figure <a href="#fig:run-scatter-time-vars"><strong>??</strong></a> below, the price relationship with age may be “multimodal”. If we were to just look at <code>taxcard</code> (the source of <code>sqft</code>), it would have a positive correlation of 20% with <code>price</code> and <code>sqft</code>. Builders and architects reporting newer homes are likely larger and more expensive.</p>
<details>
<p><summary>Click to see code generating table</summary></p>
<pre class="r"><code># Select vars
chart_vars &lt;- 
  setdiff(vars, c(&quot;qtr_median&quot;, &quot;condo&quot;))

# Separate continuous variables and log transform where appropriate
cont &lt;- 
  split_columns(copy(data[year(date_sold) &gt; 2016, ..chart_vars]))$continuous
cont[, (is_skewed) := lapply(.SD, log1p), .SDcols = is_skewed]

# Corrplot of 2017-2010
p &lt;- plot_correlation(
  cont, 
  type = &quot;continuous&quot;, 
  cor_args = list(&quot;use&quot; = &quot;pairwise.complete.obs&quot;),
  title = &quot;Correlation Plot of Key Variables - 2017-2020&quot;,
  ggtheme = theme_bw()
  ) </code></pre>
<img src="index_files/figure-html/run-continous-corr-late-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:corr-plot-late"></span>
<img src="index_files/figure-html/corr-plot-late-1.png" alt="Pairwise Correlations of Key Numeric Variables from 2017-2020" width="100%" />
<p class="caption">
Figure 4: Pairwise Correlations of Key Numeric Variables from 2017-2020
</p>
</div>
</div>
<div id="histograms" class="section level1">
<h1>Histograms</h1>
<p>There is a lot to be discovered from the histograms of our property sales. In the charts below, we removed home sold in 2004 because that was the last year before selling prices stepped up to higher levels. Our star variables, <code>assess_land</code>, <code>far</code> and <code>sqft</code>, all look Gaussian on log scale, although some might be moderately skewed. We are putting a lot of years together here, so that might be part of it. It is interesting that <code>assess_land</code> and <code>assess_improve</code> are more skewed than the other variables, and also that <code>assess_land</code> is positively, while <code>assess_building</code> is negatively skewed. There is possibly a lower boundary for <code>assess_land</code>, and also waterfront properties may fatten the tails at the higher end. Also, there is a lot more variation in <code>assess_improve</code> than in <code>assess_land</code> even though both make up half of the total assessed value on balance. All of the lots in a given zoning and acreage should have similar values so that makes sense. The majority of home sales are less than 1 acre, though there are a few areas zoned for up to 10 acres. The <code>acre</code> variable looks less bell shaped, so we will have to give this some consideration. The <code>price</code> variable looks right skewed, but only moderately so.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Select vars
chart_vars &lt;- setdiff(is_skewed, c(&quot;dom&quot;, &quot;age_renov&quot;, &quot;num_sales&quot;))

# Separate continuous variables and log transform where appropriate
cont &lt;- 
  split_columns(copy(data[between(year(date_sold), 2004, 2020), ..chart_vars]))$continuous

# Plot histogram of skewed distributions log10 transformed
p &lt;- plot_histogram(
  cont[, .SD, .SDcols = chart_vars],
  scale_x = &quot;log10&quot;,
  title = &quot;Histograms of Key Numeric Variables - Log Scale&quot;,
  ggtheme = theme_bw())</code></pre>
<img src="index_files/figure-html/run-hist-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:plot-hist"></span>
<img src="index_files/figure-html/plot-hist-1.png" alt="Histograms of Key Numeric Variables on Log10 Scale from 2003-2020" width="100%" />
<p class="caption">
Figure 5: Histograms of Key Numeric Variables on Log10 Scale from 2003-2020
</p>
</div>
<p>As mentioned above, most are “nearly normal”, but <code>acre</code>, <code>assess_improve</code> and <code>assess_land</code> show moderate skew. One place we are still not clear is how much we should worry about the remaining skew affecting our predictions. It seems likely that it would be possible to further reduce with Box Cox transformations, or dropping outliers. As we will discuss further down, further transformation of <code>acre</code> may be needed.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Select vars
trans_vars &lt;- setdiff(is_skewed, c(&quot;bath&quot;, &quot;frontage_ft&quot;, &quot;num_sales&quot;))

# Calc skew and kurtosis vectors
skew &lt;- 
  apply(
    data[between(year(date_sold), 2004, 2020), sapply(.SD, log1p), .SDcols = trans_vars], 2, e1071::skewness, na.rm = TRUE
    )
kurt &lt;-
  apply(
    data[between(year(date_sold), 2004, 2020), sapply(.SD, log1p), .SDcols = trans_vars], 2, e1071::kurtosis, na.rm = TRUE
    )

# Display skew and kurtosis dt
d &lt;- 
  data.table(
    measure = c(&quot;skew&quot;, &quot;kurtosis&quot;),
    rbind(skew[complete.cases(skew)],
          kurt[complete.cases(kurt)])
  )</code></pre>
</details>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["1","2"],["skew","kurtosis"],[0.310197773319934,-0.216128256716942],[0.170209454383539,-0.406937871900471],[1.44396225680967,1.85219475702027],[0.597456684756929,2.89260583700735],[-0.915427859583781,4.61804512475918],[0.353485204420728,0.330858490693022]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>measure<\/th>\n      <th>price<\/th>\n      <th>sqft<\/th>\n      <th>acre<\/th>\n      <th>assess_land<\/th>\n      <th>assess_improve<\/th>\n      <th>far<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"targets":2,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"targets":3,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"targets":4,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"targets":5,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"targets":6,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"targets":7,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"targets":8,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":[2,3,4,5,6,7]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":["options.columnDefs.0.render","options.columnDefs.1.render","options.columnDefs.2.render","options.columnDefs.3.render","options.columnDefs.4.render","options.columnDefs.5.render","options.columnDefs.6.render"],"jsHooks":[]}</script>
<p>Most homes were built between 50-100 years ago, though we can see the recent (re)building boom close to the y-axis. Renovations and additions mostly over the 20 and 40 years, respectively. It is hard to see here, but most homes sold have not had recent additions or renovations. None of the age variables appear to be bell-shaped, so this will take some further thinking. We filtered out sales with Assessed Ratios (<code>AR</code>) above 1.8 and below 0.75, homes which took longer than 365 days (<code>dom</code>) to sell and homes which sold for less than 75% of original list, so these charts are all truncated, and even if they were not, have natural upper and lower boundaries. The variables for <code>beds</code>, <code>bath</code> and <code>garage</code> all look bell-shaped, but we are expecting to make some transformations of these because they are so highly correlated with other variables.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Separate continuous variables and log transform where appropriate
cont &lt;- 
  split_columns(copy(data[between(year(date_sold), 2004, 2020)]))$continuous

# Choose chart vars
chart_vars &lt;- names(cont[, .SD, .SDcols = !is_skewed])

# Plot numeric not log transformed data
p &lt;- plot_histogram(
  cont[, .SD, .SDcols = chart_vars],
  title = &quot;Histograms of Key Numeric Variables - Ordinal Scale&quot;,
  ncol = 3L, 
  nrow = 4L,
  ggtheme = theme_bw())</code></pre>
<img src="index_files/figure-html/run-hist-1-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:plot-hist-1"></span>
<img src="index_files/figure-html/plot-hist-1-1.png" alt="Histograms of Key Numeric Variables on Ordinal Scale from 2004-20" width="100%" />
<p class="caption">
Figure 6: Histograms of Key Numeric Variables on Ordinal Scale from 2004-20
</p>
</div>
</div>
<div id="scatter-plots" class="section level1">
<h1>Scatter Plots</h1>
<p>In the chart below, we show the log-log scatter plot of these our key variables versus <code>price</code> (y-axis), which shows selling prices flatten out after ~1 <code>acre</code> and possibly with <code>assess_land</code>, though less defined. Variables pertaining to house size like <code>sqft</code> and <code>far</code> continue their linear rise over the full range of prices. If anything, <code>assess_improve</code> looks like <code>sqft</code>, but might even have a slight quadratic shape. We might experiment with a splines to represent <code>acre</code> to see how that goes.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Select vars
scatter_vars &lt;- setdiff(is_skewed, c(&quot;court&quot;, &quot;taxcard&quot;, &quot;num_sales&quot;))

# Scatterplot
p &lt;- plot_scatterplot(
  data[, .SD, .SDcols = scatter_vars], 
  by = &quot;price&quot;,
  scale_y = &quot;log10&quot;,
  scale_x = &quot;log10&quot;,
  ncol = 3L, 
  nrow = 2L,
  sampled_rows = 1000L,
  theme_config = list(&quot;axis.text.x&quot; = element_text(angle = 90)),
  title = &quot;Scatter Plot of Sample of Logged Numeric Variables by Log Price 2004-20&quot;,
  ggtheme = theme_bw()
  )</code></pre>
<img src="index_files/figure-html/run-scatter-log-vars-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:scatter-log-vars"></span>
<img src="index_files/figure-html/scatter-log-vars-1.png" alt="Scatter of Sample of Key Numeric Variables from 2004-2020" width="100%" />
<p class="caption">
Figure 7: Scatter of Sample of Key Numeric Variables from 2004-2020
</p>
</div>
<p>The next group of scatter plots shown below are time-related variables. The newest homes sell for the highest prices, but there appears to be some homes that hold up beyond 50 years, then several higher points before falling off again after 110 (although the data gets thin at this point). There are a considerably number of 200+ year old homes sold, but these may be historical landmarks. At least recorded in the data, there is a surge of renovations in the last 20 years, which seems to add considerably to price, though less relevant to price after about 25 years. Additions don’t seem add as noticeably to price as renovations.</p>
<p>The dates are <code>data.table</code> integer types, so the year is not visible, but there is a very wide range of selling prices represented consistently over time. A slight decline in number of higher priced sales in recent years is apparent, and there are two periods with fewer very high priced sales around the GFC and again around 2017. Homes selling below $1 million are minority over time and there have been fewer in the recent years. The <code>dom</code> variable has been truncated after 360 days, but would fall off if the chart continued.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Select vars
chart_vars &lt;- c(&quot;price&quot;, &quot;age&quot;, &quot;age_add&quot;, &quot;age_renov&quot;, &quot;date_sold&quot;, &quot;dom&quot;)

# scaterplot
p &lt;- plot_scatterplot(
  data[, ..chart_vars], 
  by = &quot;price&quot;,
  ncol=3L, 
  scale_x = &quot;log10&quot;,
  theme_config = list(&quot;axis.text.x&quot; = element_text(angle = 90)),
  title = &quot;Scatterplot of Time-Related Variables by Log Price 2004-20&quot;,
  ggtheme = theme_bw()
  )</code></pre>
<img src="index_files/figure-html/run-scatter-time-vars-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:scatter-time-vars"></span>
<img src="index_files/figure-html/scatter-time-vars-1.png" alt="Scatter Plot of Sample of Time-Related Variables from 2004-20" width="100%" />
<p class="caption">
Figure 8: Scatter Plot of Sample of Time-Related Variables from 2004-20
</p>
</div>
<p>The convention in Connecticut is that assessed value should represent 70% of the market value, so on balance, homes should sell for about 1.4 assessed value (1 / 0.7), and we have filtered out <code>AR</code> below 0.75 and above 1.8. The chart seems to peak around this level, but it is hard to say much about density in this chart which is sampled. We also see that some homes have turned over every few years, and these seem to have lower prices though above “num_sales” was slightly positively correlated with <code>price</code>. Also, we can see that the highest valued home sales sell for less than the original listing price. On balance, homes which are selling above original list are towards the lower end of sales prices, though we filtered below 75% of original list price when they took more than a year to sell.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Select vars
chart_vars &lt;- c(&quot;price&quot;, &quot;sp_olp&quot;, &quot;AR&quot;, &quot;num_sales&quot;)

# Scatterplot
p &lt;- plot_scatterplot(
  data[, ..chart_vars], 
  by = &quot;price&quot;,
  #sampled_rows = 2000L,
  theme_config = list(&quot;axis.text.x&quot; = element_text(angle = 90)),
  title = &quot;Scatterplot of Unlogged Numeric Variables by Log Price 2004-20&quot;,
  ggtheme = theme_bw()
  )</code></pre>
<img src="index_files/figure-html/run-scatter-ratios-1.png" width="100%" />
</details>
<div class="figure"><span id="fig:scatter-ratios"></span>
<img src="index_files/figure-html/scatter-ratios-1.png" alt="Scatter Plot of Sample of Other Variables Grouped by Price from 2003-2020" width="100%" />
<p class="caption">
Figure 9: Scatter Plot of Sample of Other Variables Grouped by Price from 2003-2020
</p>
</div>
</div>
<div id="catagorical-variables" class="section level1">
<h1>Catagorical Variables</h1>
<p>There are over 50 neighborhoods defined by the Town, but we have divided these into 4 larger groups with “North of Coastal” being the largest by volume, but significantly lower selling prices on average. We will talk more about school and neighborhood in greater detail in the next post as cross-sections of location seems like an important aspect of this project. There are a few other geographical variables in our data, including <code>elem</code>, <code>area</code> and <code>assess</code>. For now, the four super-neighborhoods are defined in the legend, and the categorical variables are divided by location and aggregated by sale price in Figure <a href="#fig:plot-bar">10</a> below.</p>
<p>Homes with &lt;4 beds or &lt;2.6 bathrooms were small, &lt;5 beds or &lt;6 baths were regular and above those numbers were large (denoted <code>bed_factor</code> and <code>bath_factor</code>). Home sales closing between May and August, when families try to prepare for the start of school, which were more than half the sales by volume, are marked as <code>peak</code>. Elementary schools in the four highest priced districts were denoted by <code>school</code>. The <code>build</code> variable refers to properties which have the potential to add 25% more space given their <code>far</code>, and we set up <code>zng_levels</code> based on lowest, middle and highest average sales prices. We thought that properties which sold often might sell for less, so we denoted properties selling 3 or more times as <code>high_sales</code>. Almost 1/4 of the properties by price sold this frequently in 17 years, which is surprisingly high.</p>
<p>In these charts, we can see that “North of Coastal” is less represented in the highly desired elementary school districts, and has a bigger share of smaller and older homes and less expensive zoning. Among homes which were sold, it has more on lots which could still be expanded or rebuilt into larger square footage.</p>
<details>
<p><summary>Click to see code generating charts</summary></p>
<pre class="r"><code># Separate discrete variables
disc &lt;- split_columns(copy(data))$discrete

# Helper function to filter vars with less than 15 levels
low_levels &lt;- function(var, levels = 15) length(levels(as.factor(var))) &lt; levels

# Filter cols with find_levels and drop &quot;street_suffix&quot;
bar_data &lt;-
  cbind(
    disc[!is.na(build) &amp; bed_factor != &quot;&quot; &amp; age_range != &quot;&quot;, 
             .SD[, !&quot;street_suffix_2&quot;], .SDcols = low_levels], 
    price = log10(data$price)
    )

# Reorder
setcolorder(bar_data, names(bar_data)[c(1:5, 13, 6:12, 14)])

# Barplot
p &lt;- plot_bar(
  bar_data,
  with = &quot;price&quot;,
  by = &quot;location&quot;,
  title = &quot;Bar Plots of Key Categorical Variables by Price&quot;,
  nrow = 4L,
  ncol = 2L,
  ggtheme = theme_bw())</code></pre>
<img src="index_files/figure-html/run-bar-1.png" width="100%" /><img src="index_files/figure-html/run-bar-2.png" width="100%" />
</details>
<div class="figure"><span id="fig:plot-bar"></span>
<img src="index_files/figure-html/plot-bar-1.png" alt="Bar Plot of Key Categorical Variables by Price and Location from 2003-2020" width="100%" />
<p class="caption">
Figure 10: Bar Plot of Key Categorical Variables by Price and Location from 2003-2020
</p>
</div>
<div class="figure"><span id="fig:plot-bar-2"></span>
<img src="index_files/figure-html/plot-bar-2-1.png" alt="Bar Plot of Key Categorical Variables by Price from 2003-2020 (continued)" width="100%" />
<p class="caption">
Figure 11: Bar Plot of Key Categorical Variables by Price from 2003-2020 (continued)
</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p><code>DataExplorer</code> offered structure which forced us to think carefully about our variables from a number of perspectives. We also looked at its PCA and QQ-plots, but didn’t include them in this post, and may still use <code>plot_boxplot()</code> when we consider outliers in the upcoming posts. We learned that attributes of the actual structure of home should make up relatively less of the value than most places. Once log transformed, <code>sqft</code>, <code>assess_improve</code> and <code>far</code> are all linear and highly correlated with <code>price</code>. The age and size of homes interacts within location, so that an older home in a desired might actually be dragged downwards towards the value of the land it sits on, while this might be less pronounced in other locations. Newer homes in highly desired areas might be rewarded with even higher selling prices given the scarcity. We think there are interactions as we mentioned with <code>age</code> and <code>bath</code>, <code>bed</code> and <code>taxcard</code> so it may not be easy to specify a linear model which would yield the best predictions. Some variables, like <code>acre</code> and <code>assess_land</code> which may not be linear over <code>price</code> might benefit from transformations like a spline. We have some variables which run the full time span, and others which only are available starting in 2017. Since the overall prices have been somewhat stable over time, we also might expand or reduce the time periods which we use in order to have enough data in a given location. We have done some research in Bayesian Hierarchical models, which would seem to allow us to form a “prior” about the cross-sectional behavior of neighborhoods, so this might be enable us to use our 50 neighborhoods and still have enough data in periods to get accurate predictions. In the next post, we will look more closely about the affect of location on prices over time.</p>
</div>
